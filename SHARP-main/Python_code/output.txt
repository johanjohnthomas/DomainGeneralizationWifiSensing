./count_samples.py:
```
import pickle
import os
import numpy as np

base_dir = "./doppler_traces/"
subdirs = ["AR1a_S", "AR1b_E", "AR1c_C", "AR3a_R", "AR4a_R", "AR5a_C", "AR6a_E", "AR7a_J1", "AR8a_E1", "AR9a_J1", "AR9b_J1"]
activities = "C,C1,C2,E,E1,E2,H,H1,H2,J,J1,J2,J3,L,L1,L2,L3,R,R1,R2,S,W,W1,W2"
num_antennas = 4

def expand_antennas(file_names, labels, num_antennas):
    file_names_expanded = [item for item in file_names for _ in range(num_antennas)]
    labels_expanded = [int(label) for label in labels for _ in range(num_antennas)]
    stream_ant = np.tile(np.arange(num_antennas), len(labels))
    return file_names_expanded, labels_expanded, stream_ant

# Collect all data
train_files = []
train_labels = []
val_files = []
val_labels = []
test_files = []
test_labels = []

# Load data for each subdirectory
for subdir in subdirs:
    # Load train data
    file_path = f"{base_dir}{subdir}/files_train_antennas_{activities}.txt"
    label_path = f"{base_dir}{subdir}/labels_train_antennas_{activities}.txt"
    
    if os.path.exists(file_path) and os.path.exists(label_path):
        with open(file_path, "rb") as f:
            files = pickle.load(f)
            train_files.extend(files)
        
        with open(label_path, "rb") as f:
            labels = pickle.load(f)
            # Replicate labels for each file
            train_labels.extend([labels[0]] * len(files))
    
    # Load validation data
    file_path = f"{base_dir}{subdir}/files_val_antennas_{activities}.txt"
    label_path = f"{base_dir}{subdir}/labels_val_antennas_{activities}.txt"
    
    if os.path.exists(file_path) and os.path.exists(label_path):
        with open(file_path, "rb") as f:
            files = pickle.load(f)
            val_files.extend(files)
        
        with open(label_path, "rb") as f:
            labels = pickle.load(f)
            # Replicate labels for each file
            val_labels.extend([labels[0]] * len(files))
    
    # Load test data
    file_path = f"{base_dir}{subdir}/files_test_antennas_{activities}.txt"
    label_path = f"{base_dir}{subdir}/labels_test_antennas_{activities}.txt"
    
    if os.path.exists(file_path) and os.path.exists(label_path):
        with open(file_path, "rb") as f:
            files = pickle.load(f)
            test_files.extend(files)
        
        with open(label_path, "rb") as f:
            labels = pickle.load(f)
            # Replicate labels for each file
            test_labels.extend([labels[0]] * len(files))

# Count samples before expansion
print("Before expansion:")
print(f"Train samples: {len(train_files)}")
print(f"Training labels: {np.unique(train_labels, return_counts=True)}")
print(f"Val samples: {len(val_files)}")
print(f"Val labels: {np.unique(val_labels, return_counts=True)}")
print(f"Test samples: {len(test_files)}")
print(f"Test labels: {np.unique(test_labels, return_counts=True)}")

# Expand for antennas
train_files_expanded, train_labels_expanded, _ = expand_antennas(train_files, train_labels, num_antennas)
val_files_expanded, val_labels_expanded, _ = expand_antennas(val_files, val_labels, num_antennas)
test_files_expanded, test_labels_expanded, _ = expand_antennas(test_files, test_labels, num_antennas)

# Count samples after expansion
print("\nAfter expansion:")
print(f"Train samples: {len(train_files_expanded)}")
print(f"Training labels: {np.unique(train_labels_expanded, return_counts=True)}")
print(f"Val samples: {len(val_files_expanded)}")
print(f"Val labels: {np.unique(val_labels_expanded, return_counts=True)}")
print(f"Test samples: {len(test_files_expanded)}")
print(f"Test labels: {np.unique(test_labels_expanded, return_counts=True)}")

# Count total files
total_train = len(train_files)
total_val = len(val_files)
total_test = len(test_files)

total_train_expanded = len(train_files_expanded)
total_val_expanded = len(val_files_expanded)
total_test_expanded = len(test_files_expanded)

print(f"\nTotal samples before expansion: {total_train + total_val + total_test}")
print(f"Total samples after expansion: {total_train_expanded + total_val_expanded + total_test_expanded}") ```

./dataset_utility.py:
```
"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import numpy as np
import pickle
import tensorflow as tf
import os
import shutil

tf.random.set_seed(42)
def convert_to_number(lab, csi_label_dict):
    """
    Maps a label to its corresponding index in the csi_label_dict.
    In the original implementation, each label (E, E1, E2, etc.) has its own unique index.
    """
    lab_num = np.argwhere(np.asarray(csi_label_dict) == lab)[0][0]
    return int(lab_num)

def convert_to_grouped_number(lab, csi_label_dict):
    """
    Groups activity labels by their base letter.
    For example, 'E', 'E1', 'E2' will all map to the same index (the index of 'E').
    If the base letter doesn't exist in csi_label_dict, it falls back to the original mapping.
    
    Args:
        lab: The activity label to convert (e.g., 'E1', 'J2')
        csi_label_dict: List of all possible activity labels
    
    Returns:
        int: The index corresponding to the base letter of the activity
    """
    # Extract the base letter (first character) from the label
    base_letter = lab[0] if len(lab) > 0 else lab
    
    # Look for the base letter in the label dictionary
    base_indices = np.where(np.array([label.startswith(base_letter) for label in csi_label_dict]))[0]
    
    if len(base_indices) > 0:
        # Return the first occurrence of the base letter or a label starting with it
        return int(base_indices[0])
    else:
        # Fallback to original mapping if base letter not found
        return convert_to_number(lab, csi_label_dict)

def get_label_mappings(csi_label_dict):
    """
    Returns mappings from original labels to both original and grouped indices.
    
    Args:
        csi_label_dict: List of all possible activity labels
    
    Returns:
        tuple: (original_mapping, grouped_mapping) where each is a dict mapping labels to indices
    """
    original_mapping = {label: convert_to_number(label, csi_label_dict) for label in csi_label_dict}
    grouped_mapping = {label: convert_to_grouped_number(label, csi_label_dict) for label in csi_label_dict}
    
    return original_mapping, grouped_mapping

def create_windows(csi_list, labels_list, sample_length, stride_length):
    csi_matrix_stride = []
    labels_stride = []
    for i in range(len(labels_list)):
        csi_i = csi_list[i]
        label_i = labels_list[i]
        len_csi = csi_i.shape[1]
        for ii in range(0, len_csi - sample_length, stride_length):
            csi_matrix_stride.append(csi_i[:, ii:ii+sample_length])
            labels_stride.append(label_i)
    return csi_matrix_stride, labels_stride


def create_windows_antennas(csi_list, labels_list, sample_length, stride_length, remove_mean=False):
    csi_matrix_stride = []
    labels_stride = []
    for i in range(len(labels_list)):
        csi_i = csi_list[i]  # Shape: (antennas, features, time)
        label_i = labels_list[i]
        len_csi = csi_i.shape[2]
        
        for ii in range(0, len_csi - sample_length + 1, stride_length):
            # Extract window with all antennas
            window = csi_i[:, :, ii:ii + sample_length]  # (antennas, features, time)
            if remove_mean:
                csi_mean = np.mean(window, axis=2, keepdims=True)
                window = window - csi_mean
            csi_matrix_stride.append(window)
            labels_stride.append(label_i)
    
    return csi_matrix_stride, labels_stride


def expand_antennas(file_names, labels, num_antennas):
    file_names_expanded = [item for item in file_names for _ in range(num_antennas)]
    labels_expanded = [int(label) for label in labels for _ in range(num_antennas)]  # Force Python ints
    stream_ant = np.tile(np.arange(num_antennas), len(labels))
    return file_names_expanded, labels_expanded, stream_ant


def load_data(csi_file_t):
    csi_file = csi_file_t
    if isinstance(csi_file_t, (bytes, bytearray)):
        csi_file = csi_file.decode()
    with open(csi_file, "rb") as fp:  # Unpickling
        matrix_csi = pickle.load(fp)
    matrix_csi = tf.transpose(matrix_csi, perm=[2, 1, 0])
    matrix_csi = tf.cast(matrix_csi, tf.float32)
    return matrix_csi


def create_dataset(csi_matrix_files, labels_stride, input_shape, batch_size, shuffle, cache_file, prefetch=True,
                   repeat=True):
    dataset_csi = tf.data.Dataset.from_tensor_slices((csi_matrix_files, labels_stride))
    py_funct = lambda csi_file, label: (tf.ensure_shape(tf.numpy_function(load_data, [csi_file], tf.float32),
                                                        input_shape), label)
    dataset_csi = dataset_csi.map(py_funct)
    dataset_csi = dataset_csi.cache(cache_file)
    if shuffle:
        dataset_csi = dataset_csi.shuffle(len(labels_stride))
    if repeat:
        dataset_csi = dataset_csi.repeat()
    dataset_csi = dataset_csi.batch(batch_size=batch_size)
    if prefetch:
        dataset_csi = dataset_csi.prefetch(buffer_size=1)
    return dataset_csi


def randomize_antennas(csi_data):
    stream_order = np.random.permutation(csi_data.shape[2])
    csi_data_randomized = csi_data[:, :, stream_order]
    return csi_data_randomized


def create_dataset_randomized_antennas(csi_matrix_files, labels_stride, input_shape, batch_size, shuffle, cache_file,
                                       prefetch=True, repeat=True):
    dataset_csi = tf.data.Dataset.from_tensor_slices((csi_matrix_files, labels_stride))
    py_funct = lambda csi_file, label: (tf.ensure_shape(tf.numpy_function(load_data, [csi_file], tf.float32),
                                                        input_shape), label)
    dataset_csi = dataset_csi.map(py_funct)
    dataset_csi = dataset_csi.cache(cache_file)

    if shuffle:
        dataset_csi = dataset_csi.shuffle(len(labels_stride))
    if repeat:
        dataset_csi = dataset_csi.repeat()

    randomize_funct = lambda csi_data, label: (tf.ensure_shape(tf.numpy_function(randomize_antennas, [csi_data],
                                                                                 tf.float32), input_shape), label)
    dataset_csi = dataset_csi.map(randomize_funct)

    dataset_csi = dataset_csi.batch(batch_size=batch_size)
    if prefetch:
        dataset_csi = dataset_csi.prefetch(buffer_size=1)
    return dataset_csi


def load_data_single(csi_file_t, stream_a):
    """
    Load data from a single file - Note: this function is being kept for backward compatibility
    but may not be used in the new multi-channel approach.
    """
    csi_file = csi_file_t
    if isinstance(csi_file_t, (bytes, bytearray)):
        csi_file = csi_file.decode()
    
    print(f"Loading data from file: {csi_file}")
    if not os.path.exists(csi_file):
        raise FileNotFoundError(f"Data file not found: {csi_file}")
        
    try:
        with open(csi_file, "rb") as fp:  # Unpickling
            matrix_csi = pickle.load(fp)
        
        # Check the raw loaded data
        print(f"Raw data type: {type(matrix_csi)}")
        if isinstance(matrix_csi, np.ndarray):
            print(f"Raw data shape: {matrix_csi.shape}")
            print(f"Raw data min/max: {np.min(matrix_csi)}/{np.max(matrix_csi)}")
            print(f"Is data all zeros? {np.all(matrix_csi == 0)}")
        else:
            print(f"Raw data is not numpy array: {type(matrix_csi)}")
        
        # Check the indexed data
        print(f"Stream/antenna index: {stream_a}")
        
        # STEP 1: Extract the data for this antenna/stream
        print("DATA TRANSFORMATION PROCESS:")
        print(f"  STEP 1: Extract single antenna data (index {stream_a})")
        matrix_csi_single = matrix_csi[stream_a, ...]
        print(f"  - Single antenna data shape: {matrix_csi_single.shape}")
        
        # STEP 2: Transpose the data to match expected dimensions
        print(f"  STEP 2: Transpose data from (100, 340) to (340, 100)")
        matrix_csi_single = np.transpose(matrix_csi_single)  # Explicitly use numpy transpose for clarity
        print(f"  - After transpose shape: {matrix_csi_single.shape}")
        
        # STEP 3: Add channel dimension if needed
        print(f"  STEP 3: Add channel dimension")
        if len(matrix_csi_single.shape) < 3:
            matrix_csi_single = np.expand_dims(matrix_csi_single, axis=-1)  # Shape (340, 100, 1)
            print(f"  - Final data shape with channel: {matrix_csi_single.shape}")
        
        # Verify final data
        print(f"FINAL data shape: {matrix_csi_single.shape}")
        print(f"FINAL data min/max: {np.min(matrix_csi_single)}/{np.max(matrix_csi_single)}")
        
        matrix_csi_single = tf.cast(matrix_csi_single, tf.float32)
        return matrix_csi_single
    except Exception as e:
        print(f"Error during data loading/processing: {e}")
        raise


def load_data_multi_channel(csi_file_t):
    """
    Load data from a file and process all antennas as channels.
    This is the preferred approach for processing CSI data.
    """
    csi_file = csi_file_t
    if isinstance(csi_file_t, (bytes, bytearray)):
        csi_file = csi_file.decode()
    
    print(f"Loading data from file: {csi_file}")
    if not os.path.exists(csi_file):
        raise FileNotFoundError(f"Data file not found: {csi_file}")
        
    try:
        with open(csi_file, "rb") as fp:  # Unpickling
            matrix_csi = pickle.load(fp)
        
        # Check the raw loaded data
        print(f"Raw data type: {type(matrix_csi)}")
        if isinstance(matrix_csi, np.ndarray):
            print(f"Raw data shape: {matrix_csi.shape}")
            print(f"Raw data min/max: {np.min(matrix_csi)}/{np.max(matrix_csi)}")
            print(f"Is data all zeros? {np.all(matrix_csi == 0)}")
        else:
            print(f"Raw data is not numpy array: {type(matrix_csi)}")
        
        # STEP 1: Transpose to get (340, 100, 4) - features as height, time as width, antennas as channels
        print("MULTI-CHANNEL DATA TRANSFORMATION:")
        print(f"  STEP 1: Transpose data from (4, 100, 340) to (340, 100, 4)")
        matrix_csi_multi = np.transpose(matrix_csi, (2, 1, 0))
        print(f"  - After transpose shape: {matrix_csi_multi.shape}")
        
        # STEP 2: Mean normalization across antennas (axis=2)
        print(f"  STEP 2: Mean and standard deviation normalization")
        mean = np.mean(matrix_csi_multi, axis=(0,1), keepdims=True)  # Global mean
        std = np.std(matrix_csi_multi, axis=(0,1), keepdims=True)
        matrix_csi_multi = (matrix_csi_multi - mean) / (std + 1e-9)
        print(f"  - After normalization shape: {matrix_csi_multi.shape}")
        print(f"  - After normalization min/max: {np.min(matrix_csi_multi)}/{np.max(matrix_csi_multi)}")
        
        # Verify final data
        print(f"FINAL multi-channel data shape: {matrix_csi_multi.shape}")
        print(f"FINAL data min/max: {np.min(matrix_csi_multi)}/{np.max(matrix_csi_multi)}")
        
        matrix_csi_multi = tf.cast(matrix_csi_multi, tf.float32)
        return matrix_csi_multi
    except Exception as e:
        print(f"Error during data loading/processing: {e}")
        raise

def create_dataset_single(csi_matrix_files, labels_stride, stream_ant, input_shape, batch_size, shuffle, cache_file,
                          prefetch=True, repeat=False):
    if len(csi_matrix_files) == 0:
        print("Error: Empty dataset - no files to process!")
        raise ValueError("Cannot create dataset with empty file list - please check your data paths and make sure files exist")
        
    print(f"Creating dataset with {len(csi_matrix_files)} samples")
    dataset_csi = tf.data.Dataset.from_tensor_slices((csi_matrix_files, labels_stride, stream_ant))
    
    with tf.device('/cpu:0'):
        # Clear existing cache if present
        if cache_file and os.path.exists(cache_file):
            try:
                if os.path.isfile(cache_file):
                    os.remove(cache_file)
                elif os.path.isdir(cache_file):
                    shutil.rmtree(cache_file)
            except Exception as e:
                print(f"Error clearing cache: {e}")

        # Define the map function with proper error handling
        def safe_load_data(csi_file, label, stream):
            try:
                # Print more information about the tensors
                file_path = csi_file.numpy().decode() if hasattr(csi_file, 'numpy') else str(csi_file)
                stream_value = stream.numpy() if hasattr(stream, 'numpy') else stream
                label_value = label.numpy() if hasattr(label, 'numpy') else label
                
                print(f"Processing file: {file_path}")
                print(f"Stream/antenna index: {stream_value}, Label: {label_value}")
                
                # Call the data loading function with explicit decoding of string tensor
                data = tf.numpy_function(
                    func=load_data_single,
                    inp=[csi_file, stream],
                    Tout=tf.float32
                )
                
                # Verify the returned data
                print(f"Returned data shape: {data.shape}")
                
                # Check for all zeros as a heuristic for dummy data
                if tf.reduce_all(tf.equal(data, 0)):
                    print("WARNING: Data contains all zeros!")
                    raise ValueError("Loaded data contains all zeros - potential dummy data detected")
                    
                return data, tf.squeeze(label)
            except Exception as e:
                print(f"Error in safe_load_data: {e}")
                raise

        # Apply mapping
        dataset_csi = dataset_csi.map(
            safe_load_data,
            num_parallel_calls=tf.data.AUTOTUNE
        )
        
        # Cache after mapping
        dataset_csi = dataset_csi.cache(cache_file)
        
        # Shuffle if needed
        if shuffle:
            dataset_csi = dataset_csi.shuffle(buffer_size=max(100, len(csi_matrix_files)))
        
        # Batch the data
        dataset_csi = dataset_csi.batch(batch_size)
        
        # Repeat if needed
        if repeat:
            dataset_csi = dataset_csi.repeat()
            
        # Prefetch for performance
        if prefetch:
            dataset_csi = dataset_csi.prefetch(buffer_size=tf.data.AUTOTUNE)
    
    return dataset_csi

def create_dataset_multi_channel(csi_matrix_files, labels, input_shape, batch_size, shuffle, cache_file, buffer_size=100):
    """
    Creates a tf.data.Dataset for multi-channel CSI data where each sample contains all antennas.
    
    Args:
        csi_matrix_files: List of file paths to the CSI data
        labels: List of labels corresponding to the files
        input_shape: Shape of the input data (expected to be (feature_length, sample_length, num_antennas))
        batch_size: Batch size for training
        shuffle: Whether to shuffle the dataset
        cache_file: Path to cache the dataset
        buffer_size: Buffer size for shuffling
        
    Returns:
        A tf.data.Dataset instance
    """
    # Define a function to load and preprocess a single file
    def load_and_process_file(file_path, label):
        def _parse_function(file_path, label):
            # Load the CSI matrix from the file
            with open(file_path.numpy().decode('utf-8'), 'rb') as f:
                csi_matrix = pickle.load(f)
            
            # Transpose to get (feature_length, sample_length, num_antennas)
            # From (num_antennas, sample_length, feature_length) to (feature_length, sample_length, num_antennas)
            csi_matrix = np.transpose(csi_matrix, (2, 1, 0))
            
            # Add mean and standard deviation normalization
            mean = np.mean(csi_matrix, axis=(0,1), keepdims=True)  # Global mean
            std = np.std(csi_matrix, axis=(0,1), keepdims=True)
            csi_matrix = (csi_matrix - mean) / (std + 1e-9)
            
            return csi_matrix, label
        
        # Use tf.py_function to wrap the Python function
        csi_matrix, label = tf.py_function(
            _parse_function,
            [file_path, label],
            [tf.float32, tf.int32]
        )
        
        # Set the shape information that was lost in the py_function
        csi_matrix.set_shape(input_shape)
        label.set_shape([])
        
        return csi_matrix, label
    
    # Create a dataset from the file paths and labels
    dataset = tf.data.Dataset.from_tensor_slices((csi_matrix_files, labels))
    
    # Cache the dataset for better performance
    if cache_file:
        dataset = dataset.cache(cache_file)
    
    # Shuffle the dataset if requested
    if shuffle:
        dataset = dataset.shuffle(buffer_size=buffer_size)
    
    # Map the loading function to each element
    dataset = dataset.map(load_and_process_file, 
                         num_parallel_calls=tf.data.experimental.AUTOTUNE)
    
    # Batch the dataset
    dataset = dataset.batch(batch_size)
    
    # Prefetch for better performance
    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    
    return dataset```

./check_labels.py:
```
import pickle
import os
import numpy as np

# Configuration
base_dir = "./doppler_traces/"
activities = "C,C1,C2,E,E1,E2,H,H1,H2,J,J1,J2,J3,L,L1,L2,L3,R,R1,R2,S,W,W1,W2"
subdirs = ["AR1a_S", "AR1b_E", "AR1c_C", "AR3a_R", "AR4a_R", "AR5a_C", "AR6a_E", "AR7a_J1", "AR8a_E1", "AR9a_J1", "AR9b_J1"]

def check_file(filepath):
    try:
        with open(filepath, "rb") as fp:
            data = pickle.load(fp)
            print(f"  File: {filepath}")
            print(f"  Data type: {type(data)}")
            print(f"  Length: {len(data)}")
            print(f"  Content: {data[:10]}")  # Show first 10 items
            print(f"  Unique values: {np.unique(data)}")
            print()
            return data
    except Exception as e:
        print(f"  Error reading {filepath}: {e}")
        print()
        return None

# Check each subdirectory
for subdir in subdirs:
    print(f"\nChecking subdirectory: {subdir}")
    
    # Check training files
    label_path = f"{base_dir}{subdir}/labels_train_antennas_{activities}.txt"
    file_path = f"{base_dir}{subdir}/files_train_antennas_{activities}.txt"
    
    print("Training labels:")
    labels = check_file(label_path)
    
    print("Training files:")
    files = check_file(file_path)
    
    if labels is not None and files is not None:
        print(f"  Number of labels: {len(labels)}")
        print(f"  Number of files: {len(files)}")
        if len(labels) != len(files):
            print("  WARNING: Mismatch between number of labels and files!")
            
# Print total counts
total_labels = []
total_files = []

for subdir in subdirs:
    label_path = f"{base_dir}{subdir}/labels_train_antennas_{activities}.txt"
    file_path = f"{base_dir}{subdir}/files_train_antennas_{activities}.txt"
    
    try:
        with open(label_path, "rb") as fp:
            labels = pickle.load(fp)
            total_labels.extend(labels)
        with open(file_path, "rb") as fp:
            files = pickle.load(fp)
            total_files.extend(files)
    except Exception as e:
        print(f"Error loading {subdir}: {e}")

print("\nTotal Statistics:")
print(f"Total number of labels: {len(total_labels)}")
print(f"Total number of files: {len(total_files)}")
print(f"Unique labels: {np.unique(total_labels)}")
print(f"Label distribution:")
unique_labels, counts = np.unique(total_labels, return_counts=True)
for label, count in zip(unique_labels, counts):
    print(f"  Label {label}: {count} samples") ```

./CSI_phase_sanitization_H_estimation.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
from optimization_utility import *
from os import listdir
import pickle
from plots_utility import *
import tempfile
import os

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('all_dir', help='All the files in the directory, default no', type=int, default=0)
    parser.add_argument('name', help='Name of experiment file')
    parser.add_argument('nss', help='Number of spatial streams', type=int)
    parser.add_argument('ncore', help='Number of cores', type=int)
    parser.add_argument('start_r', help='Start processing', type=int)
    parser.add_argument('end_r', help='End processing', type=int)
    args = parser.parse_args()

    exp_save_dir = args.dir
    names = []

    if args.all_dir:
        all_files = listdir(exp_save_dir)
        mat_files = []
        for i in range(len(all_files)):
            if all_files[i].endswith('.mat'):
                names.append(all_files[i][:-4])
    else:
        names.append(args.name)

    for name in names:
        name_file = './phase_processing/signal_' + name + '.txt'
        with open(name_file, "rb") as fp:  # Pickling
            signal_complete = pickle.load(fp)

        delete_idxs = np.asarray([0, 1, 2, 3, 4, 5, 127, 128, 129, 251, 252, 253, 254, 255], dtype=int)
        pilot_subcarriers = [25, 53, 89, 117, 139, 167, 203, 231]
        subcarriers_space = 2
        delta_t = 1E-7
        delta_t_refined = 5E-9
        range_refined_up = 2.5E-7
        range_refined_down = 2E-7

        start_r = args.start_r
        if args.end_r != -1:
            end_r = args.end_r
        else:
            end_r = signal_complete.shape[1]

        F_frequency = 256
        delta_f = 312.5E3
        frequency_vector_complete = np.zeros(F_frequency, )
        F_frequency_2 = F_frequency // 2
        for row in range(F_frequency_2):
            freq_n = delta_f * (row - F_frequency / 2)
            frequency_vector_complete[row] = freq_n
            freq_p = delta_f * row
            frequency_vector_complete[row + F_frequency_2] = freq_p
        frequency_vector = np.delete(frequency_vector_complete, delete_idxs)

        T = 1/delta_f
        t_min = -3E-7
        t_max = 5E-7

        T_matrix, time_matrix = build_T_matrix(frequency_vector, delta_t, t_min, t_max)
        r_length = int((t_max - t_min) / delta_t_refined)

        start_subcarrier = 0
        end_subcarrier = frequency_vector.shape[0]
        select_subcarriers = np.arange(start_subcarrier, end_subcarrier, subcarriers_space)

        n_ss = args.nss
        n_core = args.ncore
        n_tot = n_ss * n_core

        # Auxiliary data for first step
        row_T = int(T_matrix.shape[0] / subcarriers_space)
        col_T = T_matrix.shape[1]
        m = 2 * row_T
        n = 2 * col_T
        In = scipy.sparse.eye(n)
        Im = scipy.sparse.eye(m)
        On = scipy.sparse.csc_matrix((n, n))
        Onm = scipy.sparse.csc_matrix((n, m))
        P = scipy.sparse.block_diag([On, Im, On], format='csc')
        q = np.zeros(2 * n + m)
        A2 = scipy.sparse.hstack([In, Onm, -In])
        A3 = scipy.sparse.hstack([In, Onm, In])
        ones_n_matr = np.ones(n)
        zeros_n_matr = np.zeros(n)
        zeros_nm_matr = np.zeros(n + m)

        for stream in range(0, 4):
            name_file = './phase_processing/r_vector' + name + '_stream_' + str(stream) + '.txt'
            signal_considered = signal_complete[:, start_r:end_r, stream]
            r_optim = np.zeros((r_length, end_r - start_r), dtype=complex)
            Tr_matrix = np.zeros((frequency_vector_complete.shape[0], end_r - start_r), dtype=complex)

            for time_step in range(end_r - start_r):
                signal_time = signal_considered[:, time_step]
                complex_opt_r = lasso_regression_osqp_fast(signal_time, T_matrix, select_subcarriers, row_T, col_T,
                                                           Im, Onm, P, q, A2, A3, ones_n_matr, zeros_n_matr,
                                                           zeros_nm_matr)

                position_max_r = np.argmax(abs(complex_opt_r))
                time_max_r = time_matrix[position_max_r]

                T_matrix_refined, time_matrix_refined = build_T_matrix(frequency_vector, delta_t_refined,
                                                                       max(time_max_r - range_refined_down, t_min),
                                                                       min(time_max_r + range_refined_up, t_max))

                # Auxiliary data for second step
                col_T_refined = T_matrix_refined.shape[1]
                n_refined = 2 * col_T_refined
                In_refined = scipy.sparse.eye(n_refined)
                On_refined = scipy.sparse.csc_matrix((n_refined, n_refined))
                Onm_refined = scipy.sparse.csc_matrix((n_refined, m))
                P_refined = scipy.sparse.block_diag([On_refined, Im, On_refined], format='csc')
                q_refined = np.zeros(2 * n_refined + m)
                A2_refined = scipy.sparse.hstack([In_refined, Onm_refined, -In_refined])
                A3_refined = scipy.sparse.hstack([In_refined, Onm_refined, In_refined])
                ones_n_matr_refined = np.ones(n_refined)
                zeros_n_matr_refined = np.zeros(n_refined)
                zeros_nm_matr_refined = np.zeros(n_refined + m)

                complex_opt_r_refined = lasso_regression_osqp_fast(signal_time, T_matrix_refined, select_subcarriers,
                                                                   row_T, col_T_refined, Im, Onm_refined, P_refined,
                                                                   q_refined, A2_refined, A3_refined,
                                                                   ones_n_matr_refined, zeros_n_matr_refined,
                                                                   zeros_nm_matr_refined)

                position_max_r_refined = np.argmax(abs(complex_opt_r_refined))

                T_matrix_refined, time_matrix_refined = build_T_matrix(frequency_vector_complete, delta_t_refined,
                                                                       max(time_max_r - range_refined_down, t_min),
                                                                       min(time_max_r + range_refined_up, t_max))

                Tr = np.multiply(T_matrix_refined, complex_opt_r_refined)

                Tr_sum = np.sum(Tr, axis=1)

                Trr = np.multiply(Tr, np.conj(Tr[:, position_max_r_refined:position_max_r_refined + 1]))
                Trr_sum = np.sum(Trr, axis=1)

                Tr_matrix[:, time_step] = Trr_sum
                time_max_r = time_matrix_refined[position_max_r_refined]

                start_r_opt = int((time_matrix_refined[0] - t_min)/delta_t_refined)
                end_r_opt = start_r_opt + complex_opt_r_refined.shape[0]
                r_optim[start_r_opt:end_r_opt, time_step] = complex_opt_r_refined

            name_file = './phase_processing/r_vector_' + name + '_stream_' + str(stream) + '.txt'
            with tempfile.NamedTemporaryFile('wb', delete=False) as tmp:
                pickle.dump(r_optim, tmp)
                tmp_name = tmp.name
            os.rename(tmp_name, name_file)

            name_file = './phase_processing/Tr_vector_' + name + '_stream_' + str(stream) + '.txt'
            with tempfile.NamedTemporaryFile('wb', delete=False) as tmp:
                pickle.dump(Tr_matrix, tmp)
                tmp_name = tmp.name
            os.rename(tmp_name, name_file)

```

./CSI_doppler_create_dataset_train.py:
```
"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import glob
import os
import numpy as np
import pickle
import math as mt
import shutil
from dataset_utility import create_windows_antennas, convert_to_number, convert_to_grouped_number
from sklearn.model_selection import train_test_split


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('subdirs', help='Sub-directories')
    parser.add_argument('sample_lengths', help='Number of packets in a sample', type=int)
    parser.add_argument('sliding', help='Number of packet for sliding operations', type=int)
    parser.add_argument('windows_length', help='Number of samples per window', type=int)
    parser.add_argument('stride_lengths', help='Number of samples to stride', type=int)
    parser.add_argument('labels_activities', help='Labels of the activities to be considered')
    parser.add_argument('n_tot', help='Number of streams * number of antennas', type=int)
    args = parser.parse_args()

    labels_activities = args.labels_activities
    csi_label_dict = []
    for lab_act in labels_activities.split(','):
        csi_label_dict.append(lab_act)

    activities = np.asarray(labels_activities)

    n_tot = args.n_tot
    num_packets = args.sample_lengths  # 51
    middle = int(np.floor(num_packets / 2))
    list_subdir = args.subdirs  # string

    for subdir in list_subdir.split(','):
        exp_dir = args.dir + subdir + '/'

        path_train = exp_dir + 'train_antennas_' + str(activities)
        path_val = exp_dir + 'val_antennas_' + str(activities)
        path_test = exp_dir + 'test_antennas_' + str(activities)
        paths = [path_train, path_val, path_test]
        for pat in paths:
            if os.path.exists(pat):
                remove_files = glob.glob(pat + '/*')
                for f in remove_files:
                    os.remove(f)
            else:
                os.makedirs(pat, exist_ok=True)

        path_complete = exp_dir + 'complete_antennas_' + str(activities)
        if os.path.exists(path_complete):
            shutil.rmtree(path_complete)

        names = []
        all_files = os.listdir(exp_dir)
        for filename in all_files:
            if filename.endswith('.txt') and not filename.startswith('.') and '_stream_' in filename:
                names.append(filename[:-4])
        names.sort()

        csi_matrices = []
        labels = []
        lengths = []
        label = 'null'
        prev_label = label
        csi_matrix = []
        processed = False
        for i_name, name in enumerate(names):
            if i_name % n_tot == 0 and i_name != 0 and processed:
                ll = csi_matrix[0].shape[1]

                for i_ant in range(1, n_tot):
                    if ll != csi_matrix[i_ant].shape[1]:
                        break
                lengths.append(ll)
                csi_matrices.append(np.asarray(csi_matrix))
                labels.append(label)
                csi_matrix = []

            label = subdir.split("_")[-1]

            if label not in csi_label_dict:
                processed = False
                continue
            processed = True

            print(name)

            label = convert_to_grouped_number(label, csi_label_dict)
            if i_name % n_tot == 0:
                prev_label = label
            elif label != prev_label:
                print('error in ' + str(name))
                break

            name_file = exp_dir + name + '.txt'
            with open(name_file, "rb") as fp:
                stft_sum_1 = pickle.load(fp)
                
            # Convert lists to numpy arrays
            if isinstance(stft_sum_1, list):
                stft_sum_1 = np.array(stft_sum_1, dtype=np.float32)

            # Validate array type
            if stft_sum_1.dtype != np.float32 and stft_sum_1.dtype != np.float64:
                stft_sum_1 = stft_sum_1.astype(np.float32)
            stft_sum_1_mean = stft_sum_1 - np.mean(stft_sum_1, axis=0, keepdims=True)

            csi_matrix.append(stft_sum_1_mean.T)

        error = False
        if processed:
            # for the last block
            if len(csi_matrix) < n_tot:
                print('error in ' + str(name))
            ll = csi_matrix[0].shape[1]

            for i_ant in range(1, n_tot):
                if ll != csi_matrix[i_ant].shape[1]:
                    print('error in ' + str(name))
                    error = True
            if not error:
                lengths.append(ll)
                csi_matrices.append(np.asarray(csi_matrix))
                labels.append(label)

        if not error:
            lengths = np.asarray(lengths)
            if len(lengths) == 0:
                print("Error: No valid data was processed. The lengths array is empty.")
                continue  # Skip to the next subdir
            length_min = np.min(lengths)

            # Convert data to format suitable for sklearn's train_test_split
            csi_matrices_for_split = []
            
            # Prepare data for splitting
            for i in range(len(labels)):
                # Extract features from each CSI matrix up to the minimum length
                csi_matrix = csi_matrices[i][:, :, :length_min]
                # Store as a sample for splitting
                csi_matrices_for_split.append(csi_matrix)
            
            # For stratification to work correctly, we need one label per sample
            labels_for_split = np.array(labels)
            
            # Check if we have enough samples to do stratified splitting
            # We need at least 3 samples: 1 for train, 1 for val, 1 for test (minimum viable split)
            total_samples = len(csi_matrices_for_split)
            
            print(f"\nPreparing to split {total_samples} samples across train, validation, and test sets")
            
            # Get counts for each class to check if stratification is possible
            unique_labels, label_counts = np.unique(labels_for_split, return_counts=True)
            min_class_count = np.min(label_counts)
            
            print(f"Label distribution before splitting:")
            for label, count in zip(unique_labels, label_counts):
                print(f"  Class {label}: {count} samples")
            
            # MODIFY SPLITTING LOGIC TO PREVENT DUPLICATION
            # We will use a different allocation strategy based on available samples
            if total_samples >= 3 and min_class_count >= 3:
                print("Using stratified splitting with 60/20/20 ratio")
                
                # Standard stratified splitting
                X_train, X_temp, y_train, y_temp = train_test_split(
                    csi_matrices_for_split, labels_for_split, test_size=0.4, 
                    stratify=labels_for_split, random_state=42)
                
                X_val, X_test, y_val, y_test = train_test_split(
                    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)
            
            else:
                print(f"WARNING: Not enough samples for proper stratified splitting (Total: {total_samples}, Min per class: {min_class_count})")
                print(f"Using a prioritized allocation strategy:")
                
                # Instead of duplicating samples, we'll prioritize allocation based on how many samples we have
                if total_samples == 0:
                    print("CRITICAL ERROR: No samples available for this domain-activity combination")
                    print("Skipping this combination")
                    continue  # Skip to the next subdirectory
                
                elif total_samples == 1:
                    print("One sample available - allocating to training set only")
                    X_train = csi_matrices_for_split
                    y_train = labels_for_split
                    X_val = []  # Empty list
                    y_val = np.array([])
                    X_test = []  # Empty list
                    y_test = np.array([])
                    
                    print(f"Allocation: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
                
                elif total_samples == 2:
                    print("Two samples available - allocating to training and validation sets")
                    X_train = [csi_matrices_for_split[0]]
                    y_train = np.array([labels_for_split[0]])
                    X_val = [csi_matrices_for_split[1]]
                    y_val = np.array([labels_for_split[1]])
                    X_test = []  # Empty list
                    y_test = np.array([])
                    
                    print(f"Allocation: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
                
                else:  # 3+ samples but not enough per class for proper stratification
                    # Allocate 60% to train, 20% to val, 20% to test (or as close as possible)
                    train_size = max(1, int(total_samples * 0.6))
                    val_size = max(1, int(total_samples * 0.2))
                    test_size = total_samples - train_size - val_size
                    
                    # Ensure test_size is at least 1 if we have enough samples
                    if test_size < 1 and total_samples > 2:
                        val_size = total_samples - train_size - 1
                        test_size = 1
                    
                    # If we still don't have enough for test, set it to 0
                    if val_size < 1:
                        train_size = total_samples
                        val_size = 0
                        test_size = 0
                    
                    print(f"Using simple split with {train_size} train, {val_size} val, {test_size} test samples")
                    
                    # Shuffle the dataset with a fixed random seed
                    indices = np.arange(total_samples)
                    np.random.seed(42)
                    np.random.shuffle(indices)
                    
                    # Split according to calculated sizes
                    train_indices = indices[:train_size]
                    
                    if val_size > 0:
                        val_indices = indices[train_size:train_size+val_size]
                        X_val = [csi_matrices_for_split[i] for i in val_indices]
                        y_val = labels_for_split[val_indices]
                    else:
                        X_val = []  # Empty list
                        y_val = np.array([])
                    
                    if test_size > 0:
                        test_indices = indices[train_size+val_size:]
                        X_test = [csi_matrices_for_split[i] for i in test_indices]
                        y_test = labels_for_split[test_indices]
                    else:
                        X_test = []  # Empty list
                        y_test = np.array([])
                    
                    X_train = [csi_matrices_for_split[i] for i in train_indices]
                    y_train = labels_for_split[train_indices]
                    
                    print(f"Allocation: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
            
            # Prepare for window creation (convert back to the format expected by create_windows_antennas)
            csi_train = X_train
            csi_val = X_val
            csi_test = X_test
            
            # Update the labels to match the new split
            train_labels = y_train
            val_labels = y_val
            test_labels = y_test
            
            # Compute lengths for each split
            length_train = [x.shape[2] for x in csi_train]
            length_val = [x.shape[2] for x in csi_val]
            length_test = [x.shape[2] for x in csi_test]
            
            # Print out statistics about the split to verify stratification
            print(f"\nData splitting statistics:")
            print(f"Original data: {len(labels)} samples")
            print(f"After split: Train: {len(train_labels)}, Val: {len(val_labels)}, Test: {len(test_labels)}")
            
            # Check label distribution
            unique_labels = np.unique(labels_for_split)
            print("\nLabel distribution:")
            for label in unique_labels:
                orig_count = np.sum(labels_for_split == label)
                train_count = np.sum(train_labels == label)
                val_count = np.sum(val_labels == label)
                test_count = np.sum(test_labels == label)
                print(f"  Label {label}: Original: {orig_count}, Train: {train_count} ({train_count/orig_count:.2%}), Val: {val_count} ({val_count/orig_count:.2%}), Test: {test_count} ({test_count/orig_count:.2%})")

            window_length = args.windows_length  # number of windows considered
            stride_length = args.stride_lengths

            list_sets_name = ['train', 'val', 'test']
            list_sets = [csi_train, csi_val, csi_test]
            list_sets_lengths = [length_train, length_val, length_test]
            list_sets_labels = [train_labels, val_labels, test_labels]

            for set_idx in range(3):
                # Skip processing empty sets
                if len(list_sets[set_idx]) == 0:
                    print(f"Skipping {list_sets_name[set_idx]} set - no samples allocated")
                    continue

                csi_matrices_set, labels_set = create_windows_antennas(list_sets[set_idx], list_sets_labels[set_idx], window_length,
                                                                       stride_length, remove_mean=False)

                num_windows = np.floor((np.asarray(list_sets_lengths[set_idx]) - window_length) / stride_length + 1)
                if not len(csi_matrices_set) == np.sum(num_windows):
                    print('ERROR - shapes mismatch')

                names_set = []
                suffix = '.txt'
                for ii in range(len(csi_matrices_set)):
                    name_file = exp_dir + list_sets_name[set_idx] + '_antennas_' + str(activities) + '/' + \
                                str(ii) + suffix
                    names_set.append(name_file)
                    with open(name_file, "wb") as fp:  # Pickling
                        # Save the full antenna data as is
                        pickle.dump(csi_matrices_set[ii], fp)
                name_labels = exp_dir + '/labels_' + list_sets_name[set_idx] + '_antennas_' + str(activities) + suffix
                with open(name_labels, "wb") as fp:
                    # Use labels_set which contains the correct labels for each window
                    pickle.dump([int(label) for label in labels_set], fp)
                name_f = exp_dir + '/files_' + list_sets_name[set_idx] + '_antennas_' + str(activities) + suffix
                with open(name_f, "wb") as fp:  # Pickling
                    pickle.dump(names_set, fp)
                name_f = exp_dir + '/num_windows_' + list_sets_name[set_idx] + '_antennas_' + str(activities) + suffix
                with open(name_f, "wb") as fp:  # Pickling
                    pickle.dump(num_windows, fp)
```

./clean_dataset_train.py:
```
import os
import shutil

# Base directory containing all the experiment folders
base_dir = "doppler_traces/"

# Target suffix to delete (consistent across all folders)
activities_suffix = "C,C1,C2,E,E1,E2,H,H1,H2,J,J1,J2,J3,L,L1,L2,L3,R,R1,R2,S,W,W1,W2"

# Iterate through all experiment folders
for experiment_folder in os.listdir(base_dir):
    exp_path = os.path.join(base_dir, experiment_folder)
    
    if os.path.isdir(exp_path):
        # Delete complete_antennas directory
        dir_to_delete = os.path.join(exp_path, f"complete_antennas_{activities_suffix}")
        if os.path.exists(dir_to_delete):
            print(f"Deleting directory: {dir_to_delete}")
            shutil.rmtree(dir_to_delete)
        dir_to_delete = os.path.join(exp_path, f"train_antennas_{activities_suffix}")
        if os.path.exists(dir_to_delete):
            print(f"Deleting directory: {dir_to_delete}")
            shutil.rmtree(dir_to_delete)
        dir_to_delete = os.path.join(exp_path, f"test_antennas_{activities_suffix}")
        if os.path.exists(dir_to_delete):
            print(f"Deleting directory: {dir_to_delete}")
            shutil.rmtree(dir_to_delete)
        dir_to_delete = os.path.join(exp_path, f"val_antennas_{activities_suffix}")
        if os.path.exists(dir_to_delete):
            print(f"Deleting directory: {dir_to_delete}")
            shutil.rmtree(dir_to_delete)
        # Delete related files
        files_to_delete = [
            f"files_complete_antennas_{activities_suffix}.txt",
            f"labels_complete_antennas_{activities_suffix}.txt",
            f"num_windows_complete_antennas_{activities_suffix}.txt",
            f"files_test_antennas_{activities_suffix}.txt",
            f"labels_test_antennas_{activities_suffix}.txt",
            f"num_windows_test_antennas_{activities_suffix}.txt",
            f"files_train_antennas_{activities_suffix}.txt",
            f"labels_train_antennas_{activities_suffix}.txt",
            f"num_windows_train_antennas_{activities_suffix}.txt",
            f"files_val_antennas_{activities_suffix}.txt",
            f"labels_val_antennas_{activities_suffix}.txt",
            f"num_windows_val_antennas_{activities_suffix}.txt"
        ]

        
        for file_name in files_to_delete:
            file_path = os.path.join(exp_path, file_name)
            if os.path.exists(file_path):
                print(f"Deleting file: {file_path}")
                os.remove(file_path)

print("Cleanup complete!")```

./CSI_network.py:
```
"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import numpy as np
import pickle
from sklearn.metrics import confusion_matrix
import os
from dataset_utility import (create_dataset, create_dataset_randomized_antennas, 
                              create_dataset_single, create_dataset_multi_channel,
                              expand_antennas, convert_to_number, convert_to_grouped_number, 
                              get_label_mappings)
from network_utility import *
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from sklearn.utils.class_weight import compute_class_weight
import glob
import gc
import shutil
import hashlib
import sys
import time
import tensorflow as tf
from tensorflow.keras.models import load_model

"""
IMPORTANT ARCHITECTURAL NOTE:

This script has been updated to use a multi-channel approach for processing CSI data.
The key architectural changes are:

1. The model now takes inputs with shape (340, 100, 4), where:
   - 340: Feature dimension (height)
   - 100: Time dimension (width)
   - 4: All 4 antennas processed together as channels

2. The data loading pipeline has been modified to:
   - Process all 4 antennas together for each sample
   - Transpose the data from (4, 100, 340) to (340, 100, 4)
   - This allows the model to leverage cross-antenna correlations

3. Evaluation is performed on original samples, not expanded by antenna

This approach significantly improves classification accuracy by utilizing
all antenna data simultaneously instead of processing each antenna separately.
"""

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TF logging
os.environ['TF_DETERMINISTIC_OPS'] = '1'  # For reproducibility
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  # Better GPU mem management
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' 
# Now import TensorFlow
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

def compute_class_weights(labels, num_classes):
    """
    Compute class weights for imbalanced dataset using inverse frequency with smoothing.
    This approach puts more weight on underrepresented classes to help balance training.
    
    Args:
        labels: Array of class labels
        num_classes: Total number of possible classes
        
    Returns:
        dict: Dictionary mapping class indices to their weights
    """
    # Use inverse frequency with smoothing
    class_counts = np.bincount(labels, minlength=num_classes)
    weights = 1. / (class_counts + 0.1 * np.max(class_counts))
    
    # Normalize weights to prevent extremely large values
    weights = weights / np.sum(weights) * num_classes
    
    # Create a dictionary to store the weights
    class_weights = {i: weights[i] for i in range(num_classes)}
    
    # Log the weights and counts for transparency
    print("Class weights based on inverse frequency with smoothing:")
    for idx in range(num_classes):
        if idx in class_weights:
            print(f"  Class {idx}: {class_weights[idx]:.4f} (count: {class_counts[idx]})")
        else:
            # Add default weight for any missing class
            class_weights[idx] = 1.0
            print(f"  Class {idx}: {class_weights[idx]:.4f} (default - class not found)")
            
    return class_weights

def create_model(input_shape=(340, 100, 4), num_classes=6):
    """Create the CSI network model."""
    input_network = tf.keras.layers.Input(shape=input_shape)
    
    # Add L2 regularization to all convolutional layers - increased from 0.001 to 0.01
    regularizer = tf.keras.regularizers.l2(0.01)
    
    # Add initial dropout layer to reduce overfitting
    x = tf.keras.layers.Dropout(0.3, name='input_dropout')(input_network)
    
    # First branch - 3x3 convolutions
    conv3_1 = tf.keras.layers.Conv2D(3, (3, 3), padding='same', 
                                    kernel_regularizer=regularizer,
                                    name='1stconv3_1_res_a')(x)  # Now use x instead of input_network
    conv3_1 = tf.keras.layers.BatchNormalization()(conv3_1)
    conv3_1 = tf.keras.layers.Activation('relu', name='activation_1')(conv3_1)
    conv3_2 = tf.keras.layers.Conv2D(6, (3, 3), padding='same', 
                                    kernel_regularizer=regularizer,
                                    name='1stconv3_2_res_a')(conv3_1)
    conv3_2 = tf.keras.layers.BatchNormalization()(conv3_2)
    conv3_2 = tf.keras.layers.Activation('relu', name='activation_2')(conv3_2)
    conv3_3 = tf.keras.layers.Conv2D(9, (3, 3), strides=(2, 2), padding='same', 
                                    kernel_regularizer=regularizer,
                                    name='1stconv3_3_res_a')(conv3_2)
    conv3_3 = tf.keras.layers.BatchNormalization()(conv3_3)
    conv3_3 = tf.keras.layers.Activation('relu', name='activation_3')(conv3_3)
    
    # Second branch - 2x2 convolutions
    conv2_1 = tf.keras.layers.Conv2D(5, (2, 2), strides=(2, 2), padding='same', 
                                    kernel_regularizer=regularizer,
                                    name='1stconv2_1_res_a')(x)  # Now use x instead of input_network
    conv2_1 = tf.keras.layers.BatchNormalization()(conv2_1)
    conv2_1 = tf.keras.layers.Activation('relu', name='activation')(conv2_1)
    
    # Third branch - max pooling
    pool1 = tf.keras.layers.MaxPooling2D((2, 2), name='max_pooling2d')(x)  # Now use x instead of input_network
    
    # Concatenate all branches
    concat = tf.keras.layers.Concatenate(name='concatenate')([pool1, conv2_1, conv3_3])
    
    # Additional convolution
    conv4 = tf.keras.layers.Conv2D(3, (1, 1), 
                                 kernel_regularizer=regularizer,
                                 name='conv4')(concat)
    conv4 = tf.keras.layers.BatchNormalization()(conv4)
    conv4 = tf.keras.layers.Activation('relu', name='activation_4')(conv4)
    
    # Flatten and dense layers
    flat = tf.keras.layers.Flatten(name='flatten')(conv4)
    
    # Increase dropout rate from 0.6 to 0.7 for better regularization
    drop = tf.keras.layers.Dropout(0.7, name='dropout')(flat)
    
    # Add regularization to the final dense layer
    dense2 = tf.keras.layers.Dense(num_classes, 
                                 kernel_regularizer=regularizer,
                                 name='dense2')(drop)
    
    # Create model
    model = tf.keras.Model(inputs=input_network, outputs=dense2, name='csi_model')
    return model

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('subdirs', help='Subdirs for training')
    parser.add_argument('feature_length', help='Length along the feature dimension (height)', type=int)
    parser.add_argument('sample_length', help='Length along the time dimension (width)', type=int)
    parser.add_argument('channels', help='Number of channels', type=int)
    parser.add_argument('batch_size', help='Number of samples in a batch', type=int)
    parser.add_argument('num_tot', help='Number of antenna * number of spatial streams', type=int)
    parser.add_argument('name_base', help='Name base for the files')
    parser.add_argument('activities', help='Activities to be considered')
    parser.add_argument('--bandwidth', help='Bandwidth in [MHz] to select the subcarriers, can be 20, 40, 80 '
                                            '(default 80)', default=80, required=False, type=int)
    parser.add_argument('--sub_band', help='Sub_band idx in [1, 2, 3, 4] for 20 MHz, [1, 2] for 40 MHz '
                                           '(default 1)', default=1, required=False, type=int)
    parser.add_argument('--use_grouped_labels', help='Group activity labels by their base letter (e.g., E1, E2 -> E)', 
                       action='store_true', default=True, required=False)
    parser.add_argument('--undersample', help='Apply undersampling to balance class distribution', 
                       action='store_true', default=False, required=False)
    parser.add_argument('--undersample_ratio', help='Ratio for undersampling (1.0 = fully balanced, 0.0 = no balancing)',
                        default=1.0, required=False, type=float)
    args = parser.parse_args()
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(gpus)

    bandwidth = args.bandwidth
    sub_band = args.sub_band

    csi_act = args.activities
    csi_label_dict = []
    for lab_act in csi_act.split(','):
        csi_label_dict.append(lab_act)

    # Print a message about the label grouping configuration
    if args.use_grouped_labels:
        print("\nUsing GROUPED activity labels: Activities with the same base letter (e.g., E, E1, E2) will be treated as the same class")
        print("For example, E1 and E2 will both be classified as E")
        
        # Get both the original and grouped mappings
        original_mapping, grouped_mapping = get_label_mappings(csi_label_dict)
        
        # Show the grouping that will occur
        print("\nActivity label grouping:")
        base_letters = set(label[0] for label in csi_label_dict)
        for base in sorted(base_letters):
            grouped_labels = [label for label in csi_label_dict if label.startswith(base)]
            print(f"  {base}: {', '.join(grouped_labels)}")
    else:
        print("\nUsing ORIGINAL activity labels: Each activity (E, E1, E2, etc.) will be treated as a separate class")
        
    activities = np.asarray(csi_label_dict)
    
    name_base = args.name_base
    cache_prefix = f"{name_base}_{csi_act.replace(',','_')}"
    print(f"Cleaning up previous cache files: {cache_prefix}*")
    # Delete cache files
    for f in glob.glob(f"{cache_prefix}_cache*"):
        try:
            if os.path.isfile(f):
                os.remove(f)
            elif os.path.isdir(f):
                shutil.rmtree(f)
        except Exception as e:
            print(f"Could not delete {f}: {e}")

    # Delete lockfiles
    for lockfile in glob.glob("*.lockfile"):
        try:
            os.remove(lockfile)
        except Exception as e:
            print(f"Could not delete lockfile {lockfile}: {e}")

    # Configure TensorFlow caching
    #tf.data.experimental.enable_optimizations(False)

    subdirs_training = args.subdirs  # string
    labels_train = []
    all_files_train = []
    labels_val = []
    all_files_val = []
    labels_test = []
    all_files_test = []
    sample_length = args.sample_length
    feature_length = args.feature_length
    channels = args.channels
    num_antennas = args.num_tot
    input_shape = (num_antennas, sample_length, feature_length, channels)
    input_network = (sample_length, feature_length, channels)
    batch_size = args.batch_size
    output_shape = activities.shape[0]
    labels_considered = np.arange(output_shape)
    activities = activities[labels_considered]

    suffix = '.txt'

    for sdir in subdirs_training.split(','):
        exp_save_dir = args.dir + sdir + '/'
        dir_train = args.dir + sdir + '/train_antennas_' + str(csi_act) + '/'
        name_labels = args.dir + sdir + '/labels_train_antennas_' + str(csi_act) + suffix
        with open(name_labels, "rb") as fp:  # Unpickling
            domain_labels = pickle.load(fp)
            
            # Apply label grouping if enabled
            if args.use_grouped_labels:
                # Convert loaded numeric labels back to their string representation
                # then apply grouping
                str_labels = []
                for label_num in domain_labels:
                    # Look up the original activity label for this numeric index
                    for idx, act_label in enumerate(csi_label_dict):
                        if idx == label_num:
                            # Apply grouping by extracting the base letter
                            base_letter = act_label[0]
                            # Find the index of the first activity with this base letter
                            for base_idx, activity in enumerate(csi_label_dict):
                                if activity.startswith(base_letter):
                                    str_labels.append(base_idx)
                                    break
                            break
                domain_labels = str_labels
        name_f = args.dir + sdir + '/files_train_antennas_' + str(csi_act) + suffix
        with open(name_f, "rb") as fp:  # Unpickling
            domain_files = pickle.load(fp)
            # Replicate the label for each file in this domain
            domain_labels_expanded = [domain_labels[0] for _ in range(len(domain_files))]
            labels_train.extend(domain_labels_expanded)
            all_files_train.extend(domain_files)

        dir_val = args.dir + sdir + '/val_antennas_' + str(csi_act) + '/'
        name_labels = args.dir + sdir + '/labels_val_antennas_' + str(csi_act) + suffix
        with open(name_labels, "rb") as fp:  # Unpickling
            domain_labels = pickle.load(fp)
            
            # Apply label grouping if enabled
            if args.use_grouped_labels:
                # Convert loaded numeric labels back to their string representation
                # then apply grouping
                str_labels = []
                for label_num in domain_labels:
                    # Look up the original activity label for this numeric index
                    for idx, act_label in enumerate(csi_label_dict):
                        if idx == label_num:
                            # Apply grouping by extracting the base letter
                            base_letter = act_label[0]
                            # Find the index of the first activity with this base letter
                            for base_idx, activity in enumerate(csi_label_dict):
                                if activity.startswith(base_letter):
                                    str_labels.append(base_idx)
                                    break
                            break
                domain_labels = str_labels
        name_f = args.dir + sdir + '/files_val_antennas_' + str(csi_act) + suffix
        with open(name_f, "rb") as fp:  # Unpickling
            domain_files = pickle.load(fp)
            # Replicate the label for each file in this domain
            domain_labels_expanded = [domain_labels[0] for _ in range(len(domain_files))]
            labels_val.extend(domain_labels_expanded)
            all_files_val.extend(domain_files)

        dir_test = args.dir + sdir + '/test_antennas_' + str(csi_act) + '/'
        name_labels = args.dir + sdir + '/labels_test_antennas_' + str(csi_act) + suffix
        with open(name_labels, "rb") as fp:  # Unpickling
            domain_labels = pickle.load(fp)
            
            # Apply label grouping if enabled
            if args.use_grouped_labels:
                # Convert loaded numeric labels back to their string representation
                # then apply grouping
                str_labels = []
                for label_num in domain_labels:
                    # Look up the original activity label for this numeric index
                    for idx, act_label in enumerate(csi_label_dict):
                        if idx == label_num:
                            # Apply grouping by extracting the base letter
                            base_letter = act_label[0]
                            # Find the index of the first activity with this base letter
                            for base_idx, activity in enumerate(csi_label_dict):
                                if activity.startswith(base_letter):
                                    str_labels.append(base_idx)
                                    break
                            break
                domain_labels = str_labels
        name_f = args.dir + sdir + '/files_test_antennas_' + str(csi_act) + suffix
        with open(name_f, "rb") as fp:  # Unpickling
            domain_files = pickle.load(fp)
            # Replicate the label for each file in this domain
            domain_labels_expanded = [domain_labels[0] for _ in range(len(domain_files))]
            labels_test.extend(domain_labels_expanded)
            all_files_test.extend(domain_files)

    file_train_selected = [all_files_train[idx] for idx in range(len(labels_train)) if labels_train[idx] in
                           labels_considered]
    labels_train_selected = [labels_train[idx] for idx in range(len(labels_train)) if labels_train[idx] in
                             labels_considered]

    # No expansion needed - use original files and labels
    file_train_selected_expanded = file_train_selected
    labels_train_selected_expanded = labels_train_selected
    
    print("Sample labels:", labels_train_selected_expanded[:5])
    print("Label shapes:", np.array(labels_train_selected_expanded).shape)
    
    # Also use original validation and test data without expansion
    file_val_selected = [all_files_val[idx] for idx in range(len(labels_val)) if labels_val[idx] in
                         labels_considered]
    labels_val_selected = [labels_val[idx] for idx in range(len(labels_val)) if labels_val[idx] in
                           labels_considered]

    file_val_selected_expanded = file_val_selected
    labels_val_selected_expanded = labels_val_selected
        
    file_test_selected = [all_files_test[idx] for idx in range(len(labels_test)) if labels_test[idx] in
                         labels_considered]
    labels_test_selected = [labels_test[idx] for idx in range(len(labels_test)) if labels_test[idx] in
                           labels_considered]

    file_test_selected_expanded = file_test_selected
    labels_test_selected_expanded = labels_test_selected
    
    # Create a custom data generator for training instead of using TensorFlow's dataset API
    class CustomDataGenerator(tf.keras.utils.Sequence):
        def __init__(self, file_names, labels, input_shape=(340, 100, 4), batch_size=16, shuffle=True, label_mapping=None, undersample=False, undersample_ratio=1.0):
            self.file_names = file_names
            self.labels = labels
            self.input_shape = input_shape
            self.batch_size = batch_size
            self.shuffle = shuffle
            self.label_mapping = label_mapping  # Map from class indices to original labels
            self.undersample = undersample  # Whether to apply undersampling to balance classes
            self.undersample_ratio = undersample_ratio  # Ratio for undersampling (1.0 = fully balanced)
            
            # Validate file paths first
            self.validate_files()
            
            # Store original indices for reference
            self.all_indices = np.arange(len(file_names))
            
            # Apply undersampling if requested
            if self.undersample:
                print(f"Applying undersampling with ratio {self.undersample_ratio} to balance class distribution...")
                self.indices = self._create_balanced_indices()
            else:
                self.indices = self.all_indices.copy()
                
            # Shuffle if needed
            if self.shuffle:
                np.random.shuffle(self.indices)

        def validate_files(self):
            """Check if all files exist and remove any non-existent files."""
            valid_files = []
            valid_labels = []
            invalid_files = []
            
            print(f"Validating {len(self.file_names)} file paths...")
            for i, file_path in enumerate(self.file_names):
                if os.path.exists(file_path):
                    valid_files.append(file_path)
                    valid_labels.append(self.labels[i])
                else:
                    invalid_files.append(file_path)
            
            if invalid_files:
                print(f"WARNING: Found {len(invalid_files)} invalid file paths. These will be excluded.")
                print(f"First 5 invalid files: {invalid_files[:5]}")
                
                # Update file_names and labels to only include valid files
                self.file_names = valid_files
                self.labels = valid_labels
                
                if len(valid_files) == 0:
                    raise ValueError("No valid files found! Cannot create data generator.")
            else:
                print("All file paths are valid.")

        def _create_balanced_indices(self):
            """Create balanced indices by undersampling majority classes"""
            # Get all unique classes and their counts
            unique_labels, counts = np.unique(self.labels, return_counts=True)
            
            # Find the minimum class count for balancing
            min_count = np.min(counts)
            
            # Create a dictionary to store indices for each class
            class_indices = {}
            for label in unique_labels:
                class_indices[label] = np.where(np.array(self.labels) == label)[0]
            
            # Sample indices from each class according to the undersampling ratio
            balanced_indices = []
            for label, indices in class_indices.items():
                # Get the current count for this class
                current_count = len(indices)
                
                # Calculate the target count based on the original count, min count, and ratio
                if current_count <= min_count:
                    # For minority classes, keep all samples
                    target_count = current_count
                else:
                    # For majority classes, undersample based on the ratio
                    # ratio=1.0: use min_count (full undersampling)
                    # ratio=0.0: use current_count (no undersampling)
                    target_count = int(current_count - self.undersample_ratio * (current_count - min_count))
                
                # Randomly sample target_count indices from this class
                sampled_indices = np.random.choice(indices, target_count, replace=False)
                balanced_indices.extend(sampled_indices)
                
                print(f"  Class {label}: {current_count} samples -> {target_count} samples")
            
            # Convert to numpy array and return
            balanced_indices = np.array(balanced_indices)
            print(f"  Total samples after balancing: {len(balanced_indices)}")
            return balanced_indices
            
        def __len__(self):
            return int(np.ceil(len(self.indices) / self.batch_size))

        def __getitem__(self, idx):
            batch_indices = self.indices[idx*self.batch_size : (idx+1)*self.batch_size]
            batch_files = [self.file_names[i] for i in batch_indices]
            batch_labels = [self.labels[i] for i in batch_indices]
            
            batch_x = []
            for file_path in batch_files:
                try:
                    with open(file_path, 'rb') as f:
                        data = pickle.load(f)  # Shape (4, 100, 340)
                    data = np.transpose(data, (2, 1, 0))  # Transpose to (340, 100, 4)
                    batch_x.append(data)
                except FileNotFoundError:
                    print(f"ERROR: File not found during batch loading: {file_path}")
                    # Create a dummy data point with the right shape filled with zeros
                    dummy_data = np.zeros(self.input_shape)
                    batch_x.append(dummy_data)
                except Exception as e:
                    print(f"ERROR loading file {file_path}: {str(e)}")
                    dummy_data = np.zeros(self.input_shape)
                    batch_x.append(dummy_data)
            
            return np.array(batch_x), np.array(batch_labels)
            
        def on_epoch_end(self):
            """Method called at the end of every epoch to reshuffle the data and print class distribution."""
            # If undersampling is enabled, create a new balanced set for each epoch
            if self.undersample:
                self.indices = self._create_balanced_indices()
            
            # Shuffle indices if needed
            if self.shuffle:
                np.random.shuffle(self.indices)
                
            # Calculate class distribution in the shuffled data
            all_labels = [self.labels[i] for i in self.indices]
            unique_labels, counts = np.unique(all_labels, return_counts=True)
            
            # Print class distribution
            print("\n----- Class Distribution in Shuffled Data -----")
            print(f"Total samples: {len(all_labels)}")
            
            # Create a dictionary to store class distribution
            class_distribution = {}
            for label, count in zip(unique_labels, counts):
                percentage = (count / len(all_labels)) * 100
                class_distribution[int(label)] = {"count": int(count), "percentage": percentage}
            
            # Try to get access to the reverse mapping if available
            try:
                # First try to use the label mapping provided in constructor
                if self.label_mapping is not None:
                    for class_idx in sorted(class_distribution.keys()):
                        stats = class_distribution[class_idx]
                        original_label = f" (Original: {self.label_mapping.get(class_idx, 'Unknown')})"
                        print(f"Class {class_idx}{original_label}: {stats['count']} samples ({stats['percentage']:.2f}%)")
                # Fall back to global variables if available
                elif 'index_to_label' in globals():
                    for class_idx in sorted(class_distribution.keys()):
                        stats = class_distribution[class_idx]
                        original_label = f" (Original: {globals()['index_to_label'].get(class_idx, 'Unknown')})"
                        print(f"Class {class_idx}{original_label}: {stats['count']} samples ({stats['percentage']:.2f}%)")
                else:
                    # No mapping available, just print indices
                    for class_idx in sorted(class_distribution.keys()):
                        stats = class_distribution[class_idx]
                        print(f"Class {class_idx}: {stats['count']} samples ({stats['percentage']:.2f}%)")
            except Exception as e:
                # Fall back to simpler output if there's an error
                print(f"Error accessing label mapping: {e}")
                for class_idx in sorted(class_distribution.keys()):
                    stats = class_distribution[class_idx]
                    print(f"Class {class_idx}: {stats['count']} samples ({stats['percentage']:.2f}%)")
                
            print("------------------------------------------------\n")

    # Create label mapping
    unique_labels = np.unique(np.concatenate([labels_train_selected_expanded, 
                                            labels_val_selected_expanded,
                                            labels_test_selected_expanded]))
    
    print(f"Found {len(unique_labels)} unique labels across all datasets: {unique_labels}")
    
    # IMPORTANT: We are simplifying the label mapping to only include 
    # classes that actually exist in the training data
    # This prevents issues with extraneous classes like C1, C2, E2, etc.
    training_labels_only = np.unique(labels_train_selected_expanded)
    print(f"Found {len(training_labels_only)} unique labels in TRAINING data: {training_labels_only}")
    print("Using ONLY training labels for mapping to ensure model outputs match actual classes")
    
    # Ensure all labels are numeric for consistency
    numeric_labels = []
    for label in training_labels_only:  # Only use training labels
        if isinstance(label, (int, np.integer)):
            numeric_labels.append(int(label))
        elif isinstance(label, str) and label.isdigit():
            numeric_labels.append(int(label))
        else:
            # For non-numeric labels, we'll assign a unique numeric ID
            print(f"Warning: Non-numeric label '{label}' found, using numeric placeholder")
            numeric_labels.append(hash(str(label)) % 1000)  # Use hash for unique numeric ID
    
    # Create the mapping with consistent numeric types - using ONLY training labels
    label_to_index = {label: idx for idx, label in enumerate(sorted(numeric_labels))}
    index_to_label = {idx: label for label, idx in label_to_index.items()}
    
    # Print the simplified mapping
    print("\nSimplified label mapping (using only training labels):")
    for label, idx in label_to_index.items():
        print(f"  Original label {label}  Index {idx}")
    
    # Check that all validation and test labels have mappings
    missing_val_labels = [lbl for lbl in np.unique(labels_val_selected_expanded) if lbl not in label_to_index]
    missing_test_labels = [lbl for lbl in np.unique(labels_test_selected_expanded) if lbl not in label_to_index]
    
    # Add strict validation
    missing_labels = set(np.unique(labels_test_selected_expanded)) - set(label_to_index.keys())
    if missing_labels:
        print(f"Critical Error: Test contains {len(missing_labels)} labels not seen in training")
        print(f"Missing labels: {missing_labels}")
        raise ValueError("Test data contains unseen labels")
    
    if missing_val_labels or missing_test_labels:
        print("\nWARNING: Some validation/test labels are not in training data!")
        print(f"  Missing validation labels: {missing_val_labels}")
        print(f"  Missing test labels: {missing_test_labels}")
        print("  These samples will be excluded from evaluation.")
    
    # Save the simplified mappings for easier loading in test scripts
    mapping_data = {
        'label_to_index': label_to_index,
        'index_to_label': index_to_label,
        'activities': [str(a) for a in activities.tolist()]  # Ensure activities are strings
    }
    
    # Save in multiple formats and locations for better accessibility
    with open('label_mapping.pkl', 'wb') as f:
        pickle.dump(label_to_index, f)  # Original format for backward compatibility
        
    with open(f'{name_base}_label_mapping.pkl', 'wb') as f:
        pickle.dump(mapping_data, f)  # Enhanced format
    
    print(f"Simplified label mapping saved to 'label_mapping.pkl' and '{name_base}_label_mapping.pkl'")
    
    # Convert labels to indices for Keras training
    train_labels_continuous = np.array([label_to_index[label] for label in labels_train_selected_expanded])
    
    # Calculate the number of samples in each set
    num_samples_train = len(file_train_selected_expanded)
    num_samples_val = len(file_val_selected_expanded)
    num_samples_test = len(file_test_selected_expanded)
    
    # Initialize validation and test labels
    val_labels_continuous = np.array([])
    test_labels_continuous = np.array([])
    
    # Only process validation and test labels if we have data
    if num_samples_val > 0:
        val_labels_continuous = np.array([label_to_index[label] for label in labels_val_selected_expanded])
    
    if num_samples_test > 0:
        test_labels_continuous = np.array([label_to_index[label] for label in labels_test_selected_expanded])

    # Calculate class weights based on continuous indices
    num_classes = len(unique_labels)
    class_weights_raw = compute_class_weights(train_labels_continuous, num_classes)
    
    # Keras requires class_weight to be a dictionary with keys from 0 to num_classes-1
    print("\nAdjusting class weights for Keras (requires consecutive indices from 0 to num_classes-1):")
    
    # Find the actual number of unique classes in the training data
    actual_unique_classes = len(np.unique(train_labels_continuous))
    print(f"Number of unique classes in training data: {actual_unique_classes}")
    
    # Map from our indices to consecutive indices (0 to num_classes-1)
    unique_indices = sorted(list(set(train_labels_continuous)))
    index_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(unique_indices)}
    
    # Create a new class weights dictionary with proper consecutive indices
    class_weights = {}
    for i in range(actual_unique_classes):
        if i < len(unique_indices):
            old_idx = unique_indices[i]
            class_weights[i] = class_weights_raw.get(old_idx, 1.0)
        else:
            class_weights[i] = 1.0  # Default weight for any missing class
    
    # Map continuous indices to Keras continuous indices (0 to num_classes-1)
    train_labels_continuous_keras = np.array([index_mapping.get(idx, 0) for idx in train_labels_continuous])
    
    # Initialize validation and test labels for Keras
    val_labels_continuous_keras = np.array([])
    test_labels_continuous_keras = np.array([])
    
    # Only map validation and test labels if we have data
    if num_samples_val > 0:
        val_labels_continuous_keras = np.array([index_mapping.get(idx, 0) for idx in val_labels_continuous])
    
    if num_samples_test > 0:
        test_labels_continuous_keras = np.array([index_mapping.get(idx, 0) for idx in test_labels_continuous])

    # Create data generators with class balance parameters
    print("\nCreating data generators...")
    try:
        train_generator = CustomDataGenerator(
            file_train_selected_expanded, 
            train_labels_continuous_keras,
            input_shape=(340, 100, 4),
            batch_size=batch_size,
            shuffle=True,
            label_mapping=index_to_label,
            undersample=args.undersample,
            undersample_ratio=args.undersample_ratio
        )
        
        # Only create validation generator if we have validation data
        val_generator = None
        if num_samples_val > 0:
            try:
                val_generator = CustomDataGenerator(
                    file_val_selected_expanded, 
                    val_labels_continuous_keras,
                    input_shape=(340, 100, 4),
                    batch_size=batch_size,
                    shuffle=False,
                    label_mapping=index_to_label,
                    undersample=False  # No undersampling for validation
                )
                # Update num_samples_val based on validated files
                num_samples_val = len(val_generator.file_names)
                
                if num_samples_val == 0:
                    print("WARNING: All validation files were invalid! Will use validation_split instead.")
                    val_generator = None
            except ValueError as e:
                print(f"WARNING: Could not create validation generator: {e}")
                print("Will use validation_split instead.")
                val_generator = None
                num_samples_val = 0
    except ValueError as e:
        print(f"ERROR: Failed to create data generators: {e}")
        sys.exit(1)
    
    # Print information about undersampling if enabled
    if args.undersample:
        print("\nUndersampling is enabled:")
        print(f"  Ratio: {args.undersample_ratio} (1.0 = fully balanced, 0.0 = no balancing)")
        print("  Note: Undersampling reduces the number of training samples, which affects:")
        print("    1. The effective number of steps per epoch will be lower")
        print("    2. Each epoch will use a different random subset of majority classes")
        print("    3. This helps combat class imbalance but may require more epochs for convergence")
        print(f"    4. Consider increasing the number of epochs or decreasing the undersampling ratio")
        print("    5. Class weights are still applied in addition to undersampling\n")
    
    # Save the index_mapping for later use in prediction
    with open(f'{name_base}_index_mapping.pkl', 'wb') as f:
        pickle.dump({
            'index_mapping': index_mapping,
            'reverse_mapping': {new_idx: old_idx for old_idx, new_idx in index_mapping.items()}
        }, f)
    
    print(f"\nIndex mapping for Keras compatibility (saved to {name_base}_index_mapping.pkl):")
    for old_idx, new_idx in index_mapping.items():
        print(f"  Original index {old_idx}  Keras index {new_idx}")
    
    # Print Keras-compatible class weights
    print("\nKeras-compatible class weights:")
    for idx in range(actual_unique_classes):
        # Ensure we have weights for all classes 0 to actual_unique_classes-1
        if idx not in class_weights:
            class_weights[idx] = 1.0
            print(f"  Class {idx}: {class_weights[idx]:.4f} (added default weight)")
        else:
            print(f"  Class {idx}: {class_weights[idx]:.4f}")
            
    # Verify class weights have exactly the right keys
    class_weight_keys = set(class_weights.keys())
    expected_keys = set(range(actual_unique_classes))
    if class_weight_keys != expected_keys:
        print("Adjusting class weights to match expected keys exactly...")
        # Create a new dictionary with exactly the right keys
        adjusted_weights = {idx: class_weights.get(idx, 1.0) for idx in range(actual_unique_classes)}
        class_weights = adjusted_weights
        print(f"Final class weights: {class_weights}")

    # Print original mapping info for reference
    print("\nLabel to index mapping (original):")
    for label, idx in label_to_index.items():
        print(f"  Original label {label} -> Index {idx}")

    # Print sample batch shape for verification
    x_sample, y_sample = train_generator[0]
    print(f"Training batch shape: {x_sample.shape}, labels shape: {y_sample.shape}")
    print(f"Sample labels: {y_sample[:5]}")
    
    # Verify that shuffling is working by showing the first few indices
    print("\nVerifying data shuffling:")
    print(f"First 10 indices of training data: {train_generator.indices[:10]}")
    # Get a second batch to verify different indices are used
    x_sample2, y_sample2 = train_generator[1]
    print(f"Second batch sample labels: {y_sample2[:5]}")
    # Simulate an epoch end to trigger reshuffling
    train_generator.on_epoch_end()
    print(f"After reshuffling, first 10 indices: {train_generator.indices[:10]}")
    
    # Also verify that validation and test data are not being shuffled
    if val_generator is not None:
        print(f"First 10 indices of validation data (should be ordered 0-9): {val_generator.indices[:10]}")
    else:
        print("No validation generator available - will use validation_split from training data")

    # Create the model with the number of classes equal to the number of unique indices in Keras space
    actual_unique_classes = len(np.unique(train_labels_continuous_keras))
    csi_model = create_model(input_shape=(340, 100, 4), num_classes=actual_unique_classes)
    csi_model.summary()
    
    # Freeze BatchNormalization layers to prevent issues with small batch sizes
    # Use a more robust approach that can find nested BN layers
    def freeze_bn_layers(model):
        """Recursively freeze all BatchNormalization layers in the model"""
        count = 0
        for layer in model.layers:
            if isinstance(layer, tf.keras.layers.BatchNormalization):
                layer.trainable = False
                count += 1
            
            # If this layer has sub-layers (nested model), recursively freeze those too
            if hasattr(layer, 'layers') and layer.layers:
                count += freeze_bn_layers(layer)
        return count
    
    batch_norm_count = freeze_bn_layers(csi_model)
    print(f"Froze {batch_norm_count} BatchNormalization layers to improve stability with small batches")

    # Define optimizer and loss function with improved learning rate schedule
    # As recommended by expert reviewer
    initial_learning_rate = 0.001
    
    # Add learning rate decay schedule with new parameters
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate,
        decay_steps=1000,
        decay_rate=0.96,
        staircase=True)
    
    # Use the schedule with Adam optimizer
    optimiz = tf.keras.optimizers.Adam(
        learning_rate=lr_schedule,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=True  # Enable AMSGrad variant for better convergence
    )
    
    # Use sparse categorical crossentropy loss with from_logits=True
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    
    # Add additional metrics for better performance monitoring
    metrics = [
        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=2, name='top2_accuracy')
    ]
    
    # Compile the model with our improved settings
    csi_model.compile(optimizer=optimiz, loss=loss, metrics=metrics)

    # Dataset statistics - use the previously defined sample count variables
    lab, count = np.unique(labels_train_selected_expanded, return_counts=True)
    
    # Check if validation and test sets are empty
    if num_samples_val > 0:
        lab_val, count_val = np.unique(labels_val_selected_expanded, return_counts=True)
    else:
        lab_val, count_val = np.array([]), np.array([])
        print("WARNING: No validation samples available! Will use a subset of training data for validation.")
        
    if num_samples_test > 0:
        lab_test, count_test = np.unique(labels_test_selected_expanded, return_counts=True)
    else:
        lab_test, count_test = np.array([]), np.array([])
        print("WARNING: No test samples available! Evaluation will be limited to training data.")
    
    # Display dataset information 
    print(f"Training samples: {num_samples_train}")
    print(f"Validation samples: {num_samples_val}")
    print(f"Test samples: {num_samples_test}")
    
    print("\nDetailed training label distribution:")
    label_counts = {}
    for lbl in np.unique(labels_train_selected_expanded):
        count = np.sum(np.array(labels_train_selected_expanded) == lbl)
        label_counts[int(lbl)] = count
        print(f"  Label {lbl}: {count} samples")
    
    print("\nDetailed validation label distribution:")
    for lbl in np.unique(labels_val_selected_expanded):
        count = np.sum(np.array(labels_val_selected_expanded) == lbl)
        print(f"  Label {lbl}: {count} samples")
    
    print("\nDetailed test label distribution:")
    for lbl in np.unique(labels_test_selected_expanded):
        count = np.sum(np.array(labels_test_selected_expanded) == lbl)
        print(f"  Label {lbl}: {count} samples")
    
    # Fix the lab/count iterable error - more robust type checking
    print(f"\nTraining labels and counts: ")
    print(f"  lab type: {type(lab)}")
    print(f"  count type: {type(count)}")
    
    try:
        if hasattr(lab, '__iter__') and hasattr(count, '__iter__'):
            # Both are iterable
            for l, c in zip(lab, count):
                print(f"  Label {l}: {c} samples")
        elif isinstance(lab, (int, np.integer)) and isinstance(count, (int, np.integer)):
            # Both are single integers
            print(f"  Label {lab}: {count} samples")
        else:
            # Mixed types or other cases
            print(f"  Unable to print label counts due to incompatible types")
    except Exception as e:
        print(f"  Error processing label counts: {e}")
    
    # Data loading diagnostics
    print("\nData loading diagnostics:")
    print(f"  Number of subdir folders: {len(subdirs_training.split(','))}")
    print(f"  Subdirs: {subdirs_training}")
    
    # Debug raw data counts before filtering/expansion
    print("\nRaw data counts before filtering/expansion:")
    print(f"  Raw training files: {len(all_files_train)}")
    print(f"  Raw training labels: {len(labels_train)}")
    
    # Check first few raw training files
    if len(all_files_train) > 0:
        print(f"\nFirst 5 raw training files:")
        for i in range(min(5, len(all_files_train))):
            print(f"  {i}: {all_files_train[i]}")
    
    # Check first few raw training labels
    if len(labels_train) > 0:
        print(f"\nFirst 10 raw training labels:")
        for i in range(min(10, len(labels_train))):
            print(f"  {i}: {labels_train[i]}")
    
    # Check file filtering process
    print("\nChecking the file filtering process:")
    print(f"  Labels considered shape: {labels_considered.shape}")
    print(f"  Labels considered: {labels_considered}")
    
    # Count files that match the labels_considered criteria
    matching_count = sum(1 for label in labels_train if label in labels_considered)
    print(f"  Files with matching labels: {matching_count} out of {len(labels_train)}")
    
    # Debug the file selection process
    print("\nFile selection process:")
    print(f"  Selected files (after filtering): {len(file_train_selected)}")
    print(f"  Selected labels (after filtering): {len(labels_train_selected)}")
    
    # Check the expansion process
    print("\nExpansion process:")
    print(f"  Pre-expansion files: {len(file_train_selected)}")
    print(f"  num_antennas: {num_antennas}")
    print(f"  Expected post-expansion: {len(file_train_selected) * num_antennas}")
    print(f"  Actual post-expansion: {len(file_train_selected_expanded)}")
    
    # Print a sample of the expanded data
    if len(file_train_selected_expanded) > 0:
        print(f"\nSample of files (first 5):")
        for i in range(min(5, len(file_train_selected_expanded))):
            print(f"  {i}: File={file_train_selected_expanded[i]}, Label={labels_train_selected_expanded[i]}")
    
    # Examine if we're only loading one file per domain
    print("\nChecking for potential domain-specific loading patterns:")
    for domain_idx, sdir in enumerate(subdirs_training.split(',')):
        try:
            # Load the original files and check how many are selected
            name_f = args.dir + sdir + '/files_train_antennas_' + str(csi_act) + suffix
            name_labels = args.dir + sdir + '/labels_train_antennas_' + str(csi_act) + suffix
            
            with open(name_f, "rb") as fp:
                domain_files = pickle.load(fp)
            
            with open(name_labels, "rb") as fp:
                domain_labels = pickle.load(fp)
            
            # Count how many files from this domain are selected
            domain_files_in_selected = sum(1 for f in file_train_selected if any(f == df for df in domain_files))
            
            print(f"  Domain {sdir}: {len(domain_files)} total files, {domain_files_in_selected} selected")
            
            # Check if only one file per domain is selected
            if domain_files_in_selected == 1:
                print(f"    WARNING: Only one file selected from domain {sdir}!")
                # Find which file it is
                for i, f in enumerate(file_train_selected):
                    if any(f == df for df in domain_files):
                        print(f"    Selected file: {f}")
                        print(f"    Label: {labels_train_selected[i]}")
                        break
            
        except Exception as e:
            print(f"  Error analyzing domain {sdir}: {e}")
    
    # Safety check to prevent training with empty datasets
    if num_samples_train == 0:
        print("Error: No training samples found. Cannot proceed with training.")
        exit(1)
    
    # Define improved callbacks for better training
    # 1. Early stopping with increased patience and monitoring validation accuracy
    callback_stop = tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=5,  # Increased from 3 to give model more time to converge
        min_delta=0.001,  # Minimum change to qualify as improvement
        verbose=1,
        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
    )
    
    # 2. Model checkpoint to save the best model
    name_model = name_base + '_' + str(csi_act) + '_network.keras'
    callback_save = tf.keras.callbacks.ModelCheckpoint(
        name_model,
        save_freq='epoch',
        save_best_only=True,
        monitor='val_accuracy',
        verbose=1
    )
    
    # 3. Reduce learning rate when validation metrics plateau
    callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,  # Reduce learning rate by factor of 0.2 when plateau is detected
        patience=3,
        min_delta=0.001,
        verbose=1,
        min_lr=0.000001  # Minimum learning rate
    )
    
    # 4. TensorBoard logging
    log_dir = f"logs/{name_base}_{str(csi_act)}_{int(time.time())}"
    callback_tensorboard = tf.keras.callbacks.TensorBoard(
        log_dir=log_dir,
        histogram_freq=1,
        write_graph=True,
        update_freq='epoch'
    )

    # Check that the generators are working
    print("Checking data generators...")
    try:
        test_batch = train_generator[0]
        print(f"Training batch shape: {test_batch[0].shape}, labels shape: {test_batch[1].shape}")
    except Exception as e:
        print(f"Error testing data generator: {e}")
        sys.exit(1)
    
    # Determine the number of epochs based on undersampling
    if args.undersample:
        # Increase epochs when undersampling to compensate for seeing fewer samples per epoch
        num_epochs = 45  # Increased from 30 to give model more training iterations with undersampled data
        print(f"Using {num_epochs} epochs (increased from default 30) due to undersampling")
    else:
        num_epochs = 30  # Default value
    
    # If validation set is empty, create a validation split from training data
    if num_samples_val == 0 or val_generator is None:
        print("Creating validation split from training data since no validation set is available...")
        
        # Manually split the training data - use 20% for validation
        val_split_ratio = 0.2
        
        # Compute the number of samples for validation
        n_val = int(len(train_generator.file_names) * val_split_ratio)
        
        # Make a copy of the training file names and labels
        all_train_files = train_generator.file_names.copy()
        all_train_labels = train_generator.labels.copy()
        
        # Shuffle the indices to ensure a random split
        split_indices = np.arange(len(all_train_files))
        np.random.shuffle(split_indices)
        
        # Split the indices
        val_indices = split_indices[:n_val]
        train_indices = split_indices[n_val:]
        
        # Create new file and label lists for validation
        val_files = [all_train_files[i] for i in val_indices]
        val_labels = [all_train_labels[i] for i in val_indices]
        
        # Create new file and label lists for training
        new_train_files = [all_train_files[i] for i in train_indices]
        new_train_labels = [all_train_labels[i] for i in train_indices]
        
        # Create a new training generator with the reduced dataset
        train_generator = CustomDataGenerator(
            new_train_files,
            new_train_labels,
            input_shape=(340, 100, 4),
            batch_size=batch_size,
            shuffle=True,
            label_mapping=index_to_label,
            undersample=args.undersample,
            undersample_ratio=args.undersample_ratio
        )
        
        # Create a validation generator with the held-out data
        val_generator = CustomDataGenerator(
            val_files,
            val_labels,
            input_shape=(340, 100, 4),
            batch_size=batch_size,
            shuffle=False,  # No need to shuffle validation data
            label_mapping=index_to_label,
            undersample=False  # No undersampling for validation
        )
        
        print(f"Manual split created: {len(new_train_files)} training samples, {len(val_files)} validation samples")
        
        # Train with class weights and all callbacks using separate validation data
        results = csi_model.fit(
            train_generator,
            epochs=num_epochs,
            validation_data=val_generator,  # Use our manual validation generator
            callbacks=[callback_save, callback_stop, callback_reduce_lr, callback_tensorboard],
            class_weight=class_weights,
            verbose=1
        )
    else:
        # Train with class weights and all callbacks using separate validation data
        results = csi_model.fit(
            train_generator,
            epochs=num_epochs,
            validation_data=val_generator,
            callbacks=[callback_save, callback_stop, callback_reduce_lr, callback_tensorboard],
            class_weight=class_weights,
            verbose=1
        )

    # Verify label mapping consistency
    print("\nVerifying label mapping consistency:")
    # Check that all continuous labels are in the expected range
    try:
        max_label_idx = max(index_to_label.keys())
        unique_train_labels = np.unique(train_labels_continuous)
        unique_val_labels = np.unique(val_labels_continuous)
        unique_test_labels = np.unique(test_labels_continuous)
        
        print(f"Max label index in mapping: {max_label_idx}")
        print(f"Unique continuous labels in training data: {unique_train_labels}")
        print(f"Unique continuous labels in validation data: {unique_val_labels}")
        print(f"Unique continuous labels in test data: {unique_test_labels}")
        
        # Check for labels that might be missing in the mapping
        missing_train = [label for label in unique_train_labels if label not in index_to_label]
        if missing_train:
            print(f"Warning: Some training labels are missing in index_to_label mapping: {missing_train}")
            # Add missing labels with numeric values for consistency
            for label in missing_train:
                # Use the label value itself as the label to maintain numeric consistency
                index_to_label[label] = int(label) if isinstance(label, (int, np.integer)) else 0
                print(f"  Added missing label {label}  Value {index_to_label[label]}")
        
        # Verify that all original labels can be mapped back correctly
        original_train_labels = []
        for idx in unique_train_labels:
            if idx in index_to_label:
                label = index_to_label[idx]
                # Ensure numeric labels
                if isinstance(label, (int, np.integer)):
                    original_train_labels.append(int(label))
                elif isinstance(label, str) and label.isdigit():
                    original_train_labels.append(int(label))
                else:
                    original_train_labels.append(0)  # Default numeric placeholder
            else:
                print(f"Warning: Label index {idx} not found in mapping, using placeholder")
                original_train_labels.append(0)  # Use numeric placeholder
                
        print(f"Original training labels after mapping and unmapping: {original_train_labels}")
    except Exception as e:
        print(f"Warning: Error during label mapping verification: {e}")
        print("This is non-critical and training will continue.")
    
    # For inference, create a model that includes the softmax
    inference_model = tf.keras.Sequential([
        csi_model,
        tf.keras.layers.Softmax()
    ])

    # Save both models
    csi_model.save(name_model)
    inference_model.save(name_model.replace('.keras', '_inference.keras'))

    # Function to convert Keras indices back to original indices then to original labels
    def convert_predictions_to_original_labels(predicted_indices, index_to_label_map):
        """
        Convert predicted indices to their original labels using the mapping.
        Ensures all returned labels are numeric for consistency.
        """
        converted_predictions = []
        
        # First, convert Keras indices back to original indices
        reverse_mapping = {new_idx: old_idx for old_idx, new_idx in index_mapping.items()}
        
        # Ensure index_to_label dict has numeric values where possible
        for key in list(index_to_label_map.keys()):
            if not isinstance(index_to_label_map[key], (int, np.integer)):
                try:
                    index_to_label_map[key] = int(index_to_label_map[key])
                except:
                    pass  # Keep as is if not convertible
        
        for keras_idx in predicted_indices:
            # Step 1: Convert Keras index back to original index
            original_idx = reverse_mapping.get(int(keras_idx), keras_idx)
            
            # Step 2: Convert original index to original label
            if original_idx in index_to_label_map:
                label = index_to_label_map[original_idx]
                # Ensure numeric consistency
                if isinstance(label, (int, np.integer)):
                    converted_predictions.append(int(label))
                else:
                    # Try to convert to integer if it's a string representation of a number
                    try:
                        converted_predictions.append(int(label))
                    except:
                        # If it's a string like 'Unknown-5', extract the number
                        if isinstance(label, str) and label.startswith('Unknown-'):
                            try:
                                num = int(label.split('-')[1])
                                converted_predictions.append(num)
                            except:
                                converted_predictions.append(0)  # Default
                        else:
                            converted_predictions.append(0)  # Default for non-numeric
            else:
                # If index not found, use the original index as the label (numeric fallback)
                converted_predictions.append(int(original_idx))
                print(f"Warning: Index {original_idx} not found in mapping, using it as the label")
        
        return np.array(converted_predictions, dtype=np.int32)

    # EVALUATION SECTION
    print("\n" + "="*80)
    print("MODEL EVALUATION")
    print("="*80)
    print("NOTE: While we use undersampling during training for balanced class distribution,")
    print("      evaluation is performed on the complete dataset to get full performance metrics.")
    print("      This approach ensures training is balanced but evaluation is comprehensive.")
    print("="*80 + "\n")
    
    # Use inference model for predictions
    print("Evaluating on training data...")
    # Use the original labels (not expanded) since our data generator now processes all antennas together
    train_labels_original = np.array(labels_train_selected)
    # We don't need to remap these since we're comparing original labels directly
    
    # Create a separate evaluation generator without undersampling for prediction
    # This ensures we can evaluate on the entire dataset
    print("Creating evaluation generator without undersampling...")
    eval_generator = CustomDataGenerator(
        file_train_selected_expanded, 
        train_labels_continuous_keras,  # Use remapped indices
        input_shape=(340, 100, 4),
        batch_size=batch_size,
        shuffle=False,  # No need to shuffle for evaluation
        label_mapping=index_to_label,
        undersample=False  # No undersampling for evaluation
    )
    
    train_prediction_list = []
    for i in range(len(eval_generator)):
        batch_x, _ = eval_generator[i]
        batch_pred = inference_model.predict(batch_x, verbose=0)
        train_prediction_list.append(batch_pred)

    # Limit to the number of unique samples (original samples, not expanded)
    train_prediction_list = np.vstack(train_prediction_list)[:len(train_labels_original)]
    train_labels_pred_continuous = np.argmax(train_prediction_list, axis=1)
    # These are now in Keras index space, we need to convert back to original labels
    train_labels_pred = convert_predictions_to_original_labels(train_labels_pred_continuous, index_to_label)
    
    print(f"Train prediction shape: {train_prediction_list.shape}")
    print(f"Train labels shape: {train_labels_original.shape}")
    
    # Print types of labels to verify consistency
    print("\nCHECKING LABEL TYPES:")
    print(f"Train original labels type: {type(train_labels_original[0])} ({train_labels_original[0]})")
    print(f"Train predicted labels type: {type(train_labels_pred[0])} ({train_labels_pred[0]})")
    
    # Convert labels to numeric if they're not already - for confusion matrix calculation
    train_labels_original_numeric = np.array([int(label) if isinstance(label, (int, np.integer)) else 0 for label in train_labels_original])
    train_labels_pred_numeric = np.array([int(label) if isinstance(label, (int, np.integer)) else 0 for label in train_labels_pred])
    
    # Get all unique label values for consistent matrix dimensions
    all_train_labels = np.unique(np.concatenate([train_labels_original_numeric, train_labels_pred_numeric]))
    
    print(f"Train confusion matrix with {len(all_train_labels)} classes:")
    train_confusion_matrix = confusion_matrix(train_labels_original_numeric, train_labels_pred_numeric)
    print(train_confusion_matrix)
    print("\n")
    
    # Predict on validation data if available
    val_labels_original_numeric = None
    val_labels_pred_numeric = None
    
    if num_samples_val > 0:
        print("Evaluating on validation data...")
        # Use the original labels (not expanded)
        val_labels_original = np.array(labels_val_selected)
        # We don't need to remap these since we're comparing original labels directly
        
        try:
            # Create evaluation generator for validation data without undersampling
            val_eval_generator = CustomDataGenerator(
                file_val_selected_expanded,
                val_labels_continuous_keras,  # Use remapped indices
                input_shape=(340, 100, 4),
                batch_size=batch_size,
                shuffle=False,  # No need to shuffle for evaluation
                label_mapping=index_to_label,
                undersample=False  # No undersampling for evaluation
            )
            
            # Check if we have any valid files after validation
            if len(val_eval_generator.file_names) == 0:
                print("No valid validation files found. Skipping validation evaluation.")
            else:
                val_prediction_list = []
                for i in range(len(val_eval_generator)):
                    batch_x, _ = val_eval_generator[i]
                    batch_pred = inference_model.predict(batch_x, verbose=0)
                    val_prediction_list.append(batch_pred)
                
                # Limit to the number of available validation samples
                available_val_samples = min(len(val_labels_original), len(val_eval_generator.file_names))
                
                if available_val_samples > 0 and len(val_prediction_list) > 0:
                    val_prediction_list = np.vstack(val_prediction_list)[:available_val_samples]
                    val_labels_pred_continuous = np.argmax(val_prediction_list, axis=1)
                    val_labels_pred = convert_predictions_to_original_labels(val_labels_pred_continuous, index_to_label)
                    
                    print(f"Val prediction shape: {val_prediction_list.shape}")
                    print(f"Val labels shape: {val_labels_original[:available_val_samples].shape}")
                    
                    # Same for validation - use only available samples
                    val_labels_original_numeric = np.array([int(label) if isinstance(label, (int, np.integer)) else 0 
                                                          for label in val_labels_original[:available_val_samples]])
                    val_labels_pred_numeric = np.array([int(label) if isinstance(label, (int, np.integer)) else 0 
                                                      for label in val_labels_pred])
                    
                    all_val_labels = np.unique(np.concatenate([val_labels_original_numeric, val_labels_pred_numeric]))
                    
                    print(f"Validation confusion matrix with {len(all_val_labels)} classes:")
                    val_confusion_matrix = confusion_matrix(val_labels_original_numeric, val_labels_pred_numeric)
                    print(val_confusion_matrix)
                    print("\n")
                else:
                    print("No predictions generated for validation data - skipping evaluation.")
        except Exception as e:
            print(f"Error during validation evaluation: {e}")
            print("Skipping validation evaluation.")
    else:
        print("Skipping validation evaluation - no validation samples available.")
    
    # Predict on test data if available
    test_labels_original_numeric = None
    test_labels_pred_numeric = None
    
    if num_samples_test > 0:
        print("Evaluating on test data...")
        # Use the original labels (not expanded)
        test_labels_original = np.array(labels_test_selected)
        # We don't need to remap these since we're comparing original labels directly
        
        try:
            # Create evaluation generator for test data without undersampling
            test_eval_generator = CustomDataGenerator(
                file_test_selected_expanded,
                test_labels_continuous_keras,  # Use remapped indices
                input_shape=(340, 100, 4),
                batch_size=batch_size,
                shuffle=False,  # No need to shuffle for evaluation
                label_mapping=index_to_label,
                undersample=False  # No undersampling for evaluation
            )
            
            # Check if we have any valid files after validation
            if len(test_eval_generator.file_names) == 0:
                print("No valid test files found. Using training data for evaluation instead.")
                test_labels_original_numeric = train_labels_original_numeric
                test_labels_pred_numeric = train_labels_pred_numeric
            else:
                test_prediction_list = []
                for i in range(len(test_eval_generator)):
                    batch_x, _ = test_eval_generator[i]
                    batch_pred = inference_model.predict(batch_x, verbose=0)
                    test_prediction_list.append(batch_pred)
                
                # Limit to the number of available test samples
                available_test_samples = min(len(test_labels_original), len(test_eval_generator.file_names))
                
                if available_test_samples > 0 and len(test_prediction_list) > 0:
                    test_prediction_list = np.vstack(test_prediction_list)[:available_test_samples]
                    test_labels_pred_continuous = np.argmax(test_prediction_list, axis=1)
                    test_labels_pred = convert_predictions_to_original_labels(test_labels_pred_continuous, index_to_label)
                    
                    print(f"Test prediction shape: {test_prediction_list.shape}")
                    print(f"Test labels shape: {test_labels_original[:available_test_samples].shape}")

                    # Calculate metrics using original (non-expanded) labels
                    # Ensure both arrays are of the same type (numeric)
                    test_labels_original_numeric = np.array([int(l) if isinstance(l, (int, np.integer)) else 0 
                                                          for l in test_labels_original[:available_test_samples]])
                    test_labels_pred_numeric = np.array([int(l) if isinstance(l, (int, np.integer)) else 0 
                                                      for l in test_labels_pred])
                else:
                    print("No predictions generated for test data. Using training data for evaluation instead.")
                    test_labels_original_numeric = train_labels_original_numeric
                    test_labels_pred_numeric = train_labels_pred_numeric
        except Exception as e:
            print(f"Error during test evaluation: {e}")
            print("Using training data for final evaluation metrics.")
            test_labels_original_numeric = train_labels_original_numeric
            test_labels_pred_numeric = train_labels_pred_numeric
        
        # Calculate test metrics
        conf_matrix = confusion_matrix(test_labels_original_numeric, test_labels_pred_numeric)
        precision, recall, fscore, _ = precision_recall_fscore_support(
            test_labels_original_numeric,
            test_labels_pred_numeric,
            labels=np.unique(np.concatenate([test_labels_original_numeric, test_labels_pred_numeric])),
            zero_division=0
        )
        accuracy = accuracy_score(test_labels_original_numeric, test_labels_pred_numeric)
    else:
        print("Skipping test evaluation - no test samples available.")
        print("Using training data for final evaluation metrics.")
        # If no test data is available, use training data for final metrics
        test_labels_original_numeric = train_labels_original_numeric
        test_labels_pred_numeric = train_labels_pred_numeric
        
        # Calculate metrics on training data
        conf_matrix = confusion_matrix(test_labels_original_numeric, test_labels_pred_numeric)
        precision, recall, fscore, _ = precision_recall_fscore_support(
            test_labels_original_numeric,
            test_labels_pred_numeric,
            labels=np.unique(np.concatenate([test_labels_original_numeric, test_labels_pred_numeric])),
            zero_division=0
        )
        accuracy = accuracy_score(test_labels_original_numeric, test_labels_pred_numeric)
        print("NOTE: These metrics are based on training data and may overestimate model performance!")

    # Since we're now processing all antennas together, we don't need to merge antennas
    # as was done in the original code. The predictions already incorporate all antennas.
    labels_true_merge = test_labels_original_numeric
    pred_max_merge = test_labels_pred_numeric
    
    conf_matrix_max_merge = confusion_matrix(labels_true_merge, pred_max_merge, labels=np.unique(np.concatenate([labels_true_merge, pred_max_merge])))
    precision_max_merge, recall_max_merge, fscore_max_merge, _ = \
        precision_recall_fscore_support(labels_true_merge, pred_max_merge, labels=np.unique(np.concatenate([labels_true_merge, pred_max_merge])), zero_division=0)
    accuracy_max_merge = accuracy_score(labels_true_merge, pred_max_merge)

    metrics_matrix_dict = {'conf_matrix': conf_matrix,
                           'accuracy_single': accuracy,
                           'precision_single': precision,
                           'recall_single': recall,
                           'fscore_single': fscore,
                           'conf_matrix_max_merge': conf_matrix_max_merge,
                           'accuracy_max_merge': accuracy_max_merge,
                           'precision_max_merge': precision_max_merge,
                           'recall_max_merge': recall_max_merge,
                           'fscore_max_merge': fscore_max_merge}
    unique_id = hashlib.md5(f"{csi_act}_{subdirs_training}".encode()).hexdigest()[:8]
    name_file = f'./outputs/test_{unique_id}_b{bandwidth}_sb{sub_band}.txt'

    # name_file = './outputs/test_' + str(csi_act) + '_' + subdirs_training + '_band_' + str(bandwidth) + '_subband_' + \
    #             str(sub_band) + suffix
    with open(name_file, "wb") as fp:  # Pickling
        pickle.dump(metrics_matrix_dict, fp)

    # Print final results
    print("\nModel Performance Summary:")
    print(f"Test Accuracy: {accuracy * 100:.2f}%")
    print(f"Average F1-Score: {np.mean(fscore):.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    
    # Clean up resources
    tf.keras.backend.clear_session()
    gc.collect()
    
    # We need to reopen the file for writing since it was previously opened for binary writing
    name_file_txt = name_file.replace('.txt', '_metrics.txt')
    
    # Save the formatted metrics to a text file for easier reading
    with open(name_file_txt, 'w') as f:
        f.write(f"Test Metrics:\n")
        f.write(f"Average Precision: {np.mean(precision):.4f}\n")
        f.write(f"Average Recall: {np.mean(recall):.4f}\n")
        f.write(f"Average F1 Score: {np.mean(fscore):.4f}\n")
        f.write(f"Accuracy: {accuracy:.4f}\n")
        
        # Add per-class metrics if available
        f.write(f"\nPer-class Metrics:\n")
        unique_labels = np.unique(np.concatenate([test_labels_original_numeric, test_labels_pred_numeric]))
        for i, label in enumerate(unique_labels):
            if i < len(precision):
                f.write(f"Class {label}:\n")
                f.write(f"  Precision: {precision[i]:.4f}\n")
                f.write(f"  Recall: {recall[i]:.4f}\n")
                f.write(f"  F1 Score: {fscore[i]:.4f}\n")
                f.write(f"  Samples: {np.sum(test_labels_original_numeric == label)}\n")
                
        f.write(f"\nConfusion Matrix:\n")
        f.write(str(conf_matrix))
    
    # Print success message
    print(f"\nResults saved to {name_file_txt}")
    print("Training and evaluation complete!")

    
```

./network_utility.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import tensorflow as tf


def conv2d_bn(x_in, filters, kernel_size, strides=(1, 1), padding='same', activation='relu', bn=False, name=None):
    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, name=name)(x_in)
    if bn:
        bn_name = None if name is None else name + '_bn'
        x = tf.keras.layers.BatchNormalization(axis=3, name=bn_name)(x)
    if activation is not None:
        x = tf.keras.layers.Activation(activation)(x)
    return x


def reduction_a_block_small(x_in, base_name):
    x1 = tf.keras.layers.MaxPool2D((2, 2), strides=(2, 2), padding='valid')(x_in)

    x2 = conv2d_bn(x_in, 5, (2, 2), strides=(2, 2), padding='valid', name=base_name + 'conv2_1_res_a')

    x3 = conv2d_bn(x_in, 3, (1, 1), name=base_name + 'conv3_1_res_a')
    x3 = conv2d_bn(x3, 6, (2, 2), name=base_name + 'conv3_2_res_a')
    x3 = conv2d_bn(x3, 9, (4, 4), strides=(2, 2), padding='same', name=base_name + 'conv3_3_res_a')

    x4 = tf.keras.layers.Concatenate()([x1, x2, x3])
    return x4


def csi_network_inc_res(input_sh, output_sh):
    x_input = tf.keras.Input(input_sh)

    x2 = reduction_a_block_small(x_input, base_name='1st')

    x3 = conv2d_bn(x2, 3, (1, 1), name='conv4')

    x = tf.keras.layers.Flatten()(x3)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.Dense(output_sh, activation=None, name='dense2')(x)
    model = tf.keras.Model(inputs=x_input, outputs=x, name='csi_model')
    return model
```

./check_files.py:
```
import pickle
import os

base_dir = "./doppler_traces/"
subdirs = ["AR1a_S", "AR1b_E", "AR1c_C", "AR3a_R", "AR4a_R", "AR5a_C", "AR6a_E", "AR7a_J1", "AR8a_E1", "AR9a_J1", "AR9b_J1"]
activities = "C,C1,C2,E,E1,E2,H,H1,H2,J,J1,J2,J3,L,L1,L2,L3,R,R1,R2,S,W,W1,W2"

def print_file_info(file_path):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return
    
    try:
        with open(file_path, "rb") as f:
            data = pickle.load(f)
            print(f"File: {file_path}")
            print(f"  Length: {len(data)}")
            if isinstance(data, list) and len(data) > 0:
                print(f"  First few items: {data[:min(3, len(data))]}")
            print()
    except Exception as e:
        print(f"Error loading {file_path}: {e}")

total_train = 0
total_val = 0
total_test = 0

for subdir in subdirs:
    print(f"\nChecking {subdir}:")
    
    # Train files
    train_files_path = f"{base_dir}{subdir}/files_train_antennas_{activities}.txt"
    print_file_info(train_files_path)
    
    # Train labels
    train_labels_path = f"{base_dir}{subdir}/labels_train_antennas_{activities}.txt"
    print_file_info(train_labels_path)
    
    # Validation files
    val_files_path = f"{base_dir}{subdir}/files_val_antennas_{activities}.txt"
    print_file_info(val_files_path)
    
    # Validation labels
    val_labels_path = f"{base_dir}{subdir}/labels_val_antennas_{activities}.txt"
    print_file_info(val_labels_path)
    
    # Test files
    test_files_path = f"{base_dir}{subdir}/files_test_antennas_{activities}.txt"
    print_file_info(test_files_path)
    
    # Test labels
    test_labels_path = f"{base_dir}{subdir}/labels_test_antennas_{activities}.txt"
    print_file_info(test_labels_path)
    
    # Count files in directories
    try:
        train_files = len(os.listdir(f"{base_dir}{subdir}/train_antennas_{activities}/"))
        val_files = len(os.listdir(f"{base_dir}{subdir}/val_antennas_{activities}/"))
        test_files = len(os.listdir(f"{base_dir}{subdir}/test_antennas_{activities}/"))
        
        print(f"Files in train directory: {train_files}")
        print(f"Files in val directory: {val_files}")
        print(f"Files in test directory: {test_files}")
        
        total_train += train_files
        total_val += val_files
        total_test += test_files
    except Exception as e:
        print(f"Error counting files: {e}")

print("\nTotal counts:")
print(f"Total train files: {total_train}")
print(f"Total val files: {total_val}")
print(f"Total test files: {total_test}") ```

./check_empty_dirs.py:
```
#!/usr/bin/env python3
import os
import glob

# Get all directories in doppler_traces
doppler_dir = './doppler_traces/'
all_dirs = [d for d in os.listdir(doppler_dir) if os.path.isdir(os.path.join(doppler_dir, d)) and not d.startswith('complete_antennas_')]

# Check each directory for stream files
empty_dirs = []
for directory in all_dirs:
    dir_path = os.path.join(doppler_dir, directory)
    stream_files = glob.glob(f'{dir_path}/*_stream_*.txt')
    if not stream_files:
        empty_dirs.append(directory)

print('Directories without stream files:')
for d in sorted(empty_dirs):
    print(f'- {d}')
print(f'\nTotal: {len(empty_dirs)} directories without stream files out of {len(all_dirs)} total directories') ```

./run_processing.py:
```
# run_processing.py
import os
import glob
import subprocess
import sys

def main(script_name, input_dir, *args):
    # Find all AR-* directories
    dirs = glob.glob(os.path.join(input_dir, "AR-*"))
    
    for dir_path in dirs:
        cmd = ["python", script_name, dir_path + os.sep, *args]
        print("Running:", " ".join(cmd))
        subprocess.run(cmd)

if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python run_processing.py <script> <input_dir> [args...]")
        sys.exit(1)
        
    main(sys.argv[1], sys.argv[2], *sys.argv[3:])```

./test/__init__.py:
```
"""
Test package initialization file.
""" ```

./optimization_utility.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import numpy as np
import cmath as cmt
import osqp
import scipy


def convert_to_complex_osqp(real_im_n):
    len_vect = real_im_n.shape[0] // 2
    complex_n = real_im_n[:len_vect] + 1j * real_im_n[len_vect:]
    return complex_n


def build_T_matrix(frequency_vector, delta_t_, t_min_, t_max_):
    F_frequency = frequency_vector.shape[0]
    L_paths = int((t_max_ - t_min_) / delta_t_)
    T_matrix = np.zeros((F_frequency, L_paths), dtype=complex)
    time_matrix = np.zeros((L_paths,))
    for col in range(L_paths):
        time_col = t_min_ + delta_t_ * col
        time_matrix[col] = time_col
        for row in range(F_frequency):
            freq_n = frequency_vector[row]
            T_matrix[row, col] = cmt.exp(-1j * 2 * cmt.pi * freq_n * time_col)
    return T_matrix, time_matrix


def lasso_regression_osqp_fast(H_matrix_, T_matrix_, selected_subcarriers, row_T, col_T, Im, Onm, P, q, A2, A3,
                               ones_n_matr, zeros_n_matr, zeros_nm_matr):
    # time_start = time.time()
    T_matrix_selected = T_matrix_[selected_subcarriers, :]
    H_matrix_selected = H_matrix_[selected_subcarriers]

    T_matrix_real = np.zeros((2*row_T, 2*col_T))
    T_matrix_real[:row_T, :col_T] = np.real(T_matrix_selected)
    T_matrix_real[row_T:, col_T:] = np.real(T_matrix_selected)
    T_matrix_real[row_T:, :col_T] = np.imag(T_matrix_selected)
    T_matrix_real[:row_T, col_T:] = - np.imag(T_matrix_selected)

    H_matrix_real = np.zeros((2*row_T))
    H_matrix_real[:row_T] = np.real(H_matrix_selected)
    H_matrix_real[row_T:] = np.imag(H_matrix_selected)

    n = col_T*2

    # OSQP data
    A = scipy.sparse.vstack([scipy.sparse.hstack([T_matrix_real, -Im, Onm.T]),
                             A2,
                             A3], format='csc')
    l = np.hstack([H_matrix_real, - np.inf * ones_n_matr, zeros_n_matr])
    u = np.hstack([H_matrix_real, zeros_n_matr, np.inf * ones_n_matr])

    # Create an OSQP object
    prob = osqp.OSQP()

    # Setup workspace
    prob.setup(P, q, A, l, u, warm_start=True, verbose=False)

    # Update linear cost
    lambd = 1E-1
    q_new = np.hstack([zeros_nm_matr, lambd * ones_n_matr])
    prob.update(q=q_new)

    # Solve
    res = prob.solve()

    x_out = res.x
    x_out_cut = x_out[:n]

    r_opt = convert_to_complex_osqp(x_out_cut)
    return r_opt
```

./parallel_h_estimation.py:
```
import os
import glob
import subprocess
from multiprocessing import Pool

# Configuration from Makefile
PHASE_PROCESSING_DIR = "./phase_processing/"
NSS = 1
NCORE = 4
START_IDX = 0
END_IDX = -1

def process_signal_file(signal_file):
    name = os.path.basename(signal_file).replace("signal_", "").replace(".txt", "")
    
    # Check if all 8 output files exist (4 r_vector and 4 Tr_vector)
    all_files_exist = True
    for stream in range(4):
        r_file = os.path.join(PHASE_PROCESSING_DIR, f"r_vector_{name}_stream_{stream}.txt")
        tr_file = os.path.join(PHASE_PROCESSING_DIR, f"Tr_vector_{name}_stream_{stream}.txt")
        if not (os.path.exists(r_file) and os.path.exists(tr_file)):
            all_files_exist = False
            break
    
    if all_files_exist:
        return f"Skipped {name}, already processed."
    
    # Run processing if any files are missing
    cmd = [
        "python", 
        "CSI_phase_sanitization_H_estimation.py",
        PHASE_PROCESSING_DIR,
        "0",
        name,
        str(NSS),
        str(NCORE),
        str(START_IDX),
        str(END_IDX)
    ]
    print(f"Processing {name}...")
    subprocess.run(cmd, check=True)
    return f"Completed {name}"

if __name__ == "__main__":
    # Get list of signal files
    signal_files = glob.glob(os.path.join(PHASE_PROCESSING_DIR, "signal_*.txt"))
    
    # Use all available cores
    num_processes = os.cpu_count()
    
    print(f"Starting parallel processing with {num_processes} processes...")
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_signal_file, signal_files)
    
    for result in results:
        print(result)
    print("All H estimations completed!")```

./CSI_network_metrics.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import numpy as np
import pickle


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('name_file', help='Name of the file')
    parser.add_argument('activities', help='Activities to be considered')
    args = parser.parse_args()

    name_file = args.name_file  # string
    csi_act = args.activities
    activities = []
    for lab_act in csi_act.split(','):
        activities.append(lab_act)
    activities = np.asarray(activities)

    folder_name = './outputs/'

    name_file = folder_name + name_file + '.txt'

    with open(name_file, "rb") as fp:  # Pickling
        conf_matrix_dict = pickle.load(fp)

    conf_matrix = conf_matrix_dict['conf_matrix']
    confusion_matrix_normaliz_row = np.transpose(conf_matrix / np.sum(conf_matrix, axis=1).reshape(-1, 1))
    accuracies = np.diag(confusion_matrix_normaliz_row)
    accuracy = conf_matrix_dict['accuracy_single']
    precision = conf_matrix_dict['precision_single']
    recall = conf_matrix_dict['recall_single']
    fscore = conf_matrix_dict['fscore_single']
    average_prec = np.mean(precision)
    average_rec = np.mean(recall)
    average_f = np.mean(recall)
    print('single antenna - average accuracy %f, average precision %f, average recall %f, average fscore %f'
          % (accuracy, average_prec, average_rec, average_f))
    print('fscores - empty %f, sitting %f, walking %f, running %f, jumping %f'
          % (fscore[0], fscore[1], fscore[2], fscore[3], fscore[4]))
    print('average fscore %f' % (np.mean(fscore)))
    print('accuracies - empty %f, sitting %f, walking %f, running %f, jumping %f'
          % (accuracies[0], accuracies[1], accuracies[2], accuracies[3], accuracies[4]))

    conf_matrix_max_merge = conf_matrix_dict['conf_matrix_max_merge']
    conf_matrix_max_merge_normaliz_row = np.transpose(conf_matrix_max_merge /
                                                      np.sum(conf_matrix_max_merge, axis=1).reshape(-1, 1))
    accuracies_max_merge = np.diag(conf_matrix_max_merge_normaliz_row)
    accuracy_max_merge = conf_matrix_dict['accuracy_max_merge']
    precision_max_merge = conf_matrix_dict['precision_max_merge']
    recall_max_merge = conf_matrix_dict['recall_max_merge']
    fscore_max_merge = conf_matrix_dict['fscore_max_merge']
    average_max_merge_prec = np.mean(precision_max_merge)
    average_max_merge_rec = np.mean(recall_max_merge)
    average_max_merge_f = np.mean(fscore_max_merge)
    print('\n-- FINAL DECISION --')
    print('max-merge - average accuracy %f, average precision %f, average recall %f, average fscore %f'
          % (accuracy_max_merge, average_max_merge_prec, average_max_merge_rec, average_max_merge_f))
    print('fscores - empty %f, sitting %f, walking %f, running %f, jumping %f'
          % (fscore_max_merge[0], fscore_max_merge[1], fscore_max_merge[2], fscore_max_merge[3], fscore_max_merge[4]))
    print('accuracies - empty %f, sitting %f, walking %f, running %f, jumping %f'
          % (accuracies_max_merge[0], accuracies_max_merge[1], accuracies_max_merge[2], accuracies_max_merge[3],
             accuracies_max_merge[4]))

    # performance assessment by changing the number of monitor antennas
    name_file = folder_name + 'change_number_antennas_' + args.name_file + '.txt'
    with open(name_file, "rb") as fp:  # Pickling
        metrics_matrix_dict = pickle.load(fp)

    average_accuracy_change_num_ant = metrics_matrix_dict['average_accuracy_change_num_ant']
    average_fscore_change_num_ant = metrics_matrix_dict['average_fscore_change_num_ant']
    print('\naccuracies - one antenna %f, two antennas %f, three antennas %f, four antennas %f'
          % (average_accuracy_change_num_ant[0], average_accuracy_change_num_ant[1], average_accuracy_change_num_ant[2],
             average_accuracy_change_num_ant[3]))
    print('fscores - one antenna %f, two antennas %f, three antennas %f, four antennas %f'
          % (average_fscore_change_num_ant[0], average_fscore_change_num_ant[1], average_fscore_change_num_ant[2],
             average_fscore_change_num_ant[3]))
```

./CSI_doppler_computation.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import numpy as np
import scipy.io as sio
import math as mt
from scipy.fftpack import fft
from scipy.fftpack import fftshift
from scipy.signal.windows import hann
import pickle
import os


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('subdirs', help='Sub-directories')
    parser.add_argument('dir_doppler', help='Directory to save the Doppler data')
    parser.add_argument('start', help='Start processing', type=int)
    parser.add_argument('end', help='End processing (samples from the end)', type=int)
    parser.add_argument('sample_length', help='Number of packet in a sample', type=int)
    parser.add_argument('sliding', help='Number of packet for sliding operations', type=int)
    parser.add_argument('noise_level', help='Level for the noise to be removed', type=float)
    parser.add_argument('--bandwidth', help='Bandwidth in [MHz] to select the subcarriers, can be 20, 40, 80 '
                                            '(default 80)', default=80, required=False, type=int)
    parser.add_argument('--sub_band', help='Sub_band idx in [1, 2, 3, 4] for 20 MHz, [1, 2] for 40 MHz '
                                           '(default 1)', default=1, required=False, type=int)
    args = parser.parse_args()

    num_symbols = args.sample_length  # 51
    middle = int(mt.floor(num_symbols / 2))

    Tc = 6e-3
    fc = 5e9
    v_light = 3e8
    delta_v = round(v_light / (Tc * fc * num_symbols), 3)

    sliding = args.sliding
    noise_lev = args.noise_level
    bandwidth = args.bandwidth
    sub_band = args.sub_band

    list_subdir = args.subdirs

    for subdir in list_subdir.split(','):
        path_doppler = args.dir_doppler + subdir
        if not os.path.exists(path_doppler):
            os.mkdir(path_doppler)

        exp_dir = args.dir + subdir + '/'

        names = []
        all_files = os.listdir(exp_dir)
        for i in range(len(all_files)):
            names.append(all_files[i][:-4])

        for name in names:
            path_doppler_name = path_doppler + '/' + name + '.txt'
            if os.path.exists(path_doppler_name):
                continue

            print(path_doppler_name)
            name_file = exp_dir + name + '.mat'
            mdic = sio.loadmat(name_file)
            csi_matrix_processed = mdic['csi_matrix_processed']

            csi_matrix_processed = csi_matrix_processed[args.start:-args.end, :, :]

            csi_matrix_processed[:, :, 0] = csi_matrix_processed[:, :, 0] / np.mean(csi_matrix_processed[:, :, 0],
                                                                                    axis=1,  keepdims=True)

            csi_matrix_complete = csi_matrix_processed[:, :, 0]*np.exp(1j*csi_matrix_processed[:, :, 1])

            if bandwidth == 40:
                if sub_band == 1:
                    selected_subcarriers_idxs = np.arange(0, 117, 1)
                elif sub_band == 2:
                    selected_subcarriers_idxs = np.arange(128, 245, 1)
                num_selected_subcarriers = selected_subcarriers_idxs.shape[0]
                csi_matrix_complete = csi_matrix_complete[:, selected_subcarriers_idxs]
            elif bandwidth == 20:
                if sub_band == 1:
                    selected_subcarriers_idxs = np.arange(0, 57, 1)
                elif sub_band == 2:
                    selected_subcarriers_idxs = np.arange(60, 117, 1)
                elif sub_band == 3:
                    selected_subcarriers_idxs = np.arange(128, 185, 1)
                elif sub_band == 4:
                    selected_subcarriers_idxs = np.arange(188, 245, 1)
                num_selected_subcarriers = selected_subcarriers_idxs.shape[0]
                csi_matrix_complete = csi_matrix_complete[:, selected_subcarriers_idxs]

            csi_d_profile_list = []
            for i in range(0, csi_matrix_complete.shape[0]-num_symbols, sliding):
                csi_matrix_cut = csi_matrix_complete[i:i+num_symbols, :]
                csi_matrix_cut = np.nan_to_num(csi_matrix_cut)

                hann_window = np.expand_dims(hann(num_symbols), axis=-1)
                csi_matrix_wind = np.multiply(csi_matrix_cut, hann_window)
                csi_doppler_prof = fft(csi_matrix_wind, n=100, axis=0)
                csi_doppler_prof = fftshift(csi_doppler_prof, axes=0)

                csi_d_map = np.abs(csi_doppler_prof * np.conj(csi_doppler_prof))
                csi_d_map = np.sum(csi_d_map, axis=1)
                csi_d_profile_list.append(csi_d_map)
            csi_d_profile_array = np.asarray(csi_d_profile_list)
            csi_d_profile_array_max = np.max(csi_d_profile_array, axis=1, keepdims=True)
            csi_d_profile_array = csi_d_profile_array/csi_d_profile_array_max
            csi_d_profile_array[csi_d_profile_array < mt.pow(10, noise_lev)] = mt.pow(10, noise_lev)

            with open(path_doppler_name, "wb") as fp:  # Pickling
                csi_d_profile_array = np.array(csi_d_profile_array, dtype=np.float32)
                pickle.dump(csi_d_profile_array, fp)
```

./__init__.py:
```
"""
Package initialization file for the SHARP project.
""" ```

./cleanup_empty_dirs.py:
```
#!/usr/bin/env python3
import os
import glob
import shutil

# Get all directories in doppler_traces
doppler_dir = './doppler_traces/'
all_dirs = [d for d in os.listdir(doppler_dir) if os.path.isdir(os.path.join(doppler_dir, d)) and not d.startswith('complete_antennas_')]

# Check each directory for stream files
empty_dirs = []
for directory in all_dirs:
    dir_path = os.path.join(doppler_dir, directory)
    stream_files = glob.glob(f'{dir_path}/*_stream_*.txt')
    if not stream_files:
        empty_dirs.append(directory)

# Print the directories that will be removed
print('The following directories will be removed:')
for d in sorted(empty_dirs):
    print(f'- {d}')

# Ask for confirmation
confirmation = input(f'\nAre you sure you want to remove these {len(empty_dirs)} directories? (yes/no): ')

# Remove directories if confirmed
if confirmation.lower() == 'yes':
    for directory in empty_dirs:
        dir_path = os.path.join(doppler_dir, directory)
        shutil.rmtree(dir_path)
        print(f'Removed: {dir_path}')
    print(f'\nSuccessfully removed {len(empty_dirs)} empty directories.')
else:
    print('Operation cancelled. No directories were removed.') ```

./CSI_phase_sanitization_signal_preprocessing.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import numpy as np
import scipy.io as sio
from os import listdir
import pickle
from os import path


def hampel_filter(input_matrix, window_size, n_sigmas=3):
    n = input_matrix.shape[1]
    new_matrix = np.zeros_like(input_matrix)
    k = 1.4826  # scale factor for Gaussian distribution

    for ti in range(n):
        start_time = max(0, ti - window_size)
        end_time = min(n, ti + window_size)
        x0 = np.nanmedian(input_matrix[:, start_time:end_time], axis=1, keepdims=True)
        s0 = k * np.nanmedian(np.abs(input_matrix[:, start_time:end_time] - x0), axis=1)
        mask = (np.abs(input_matrix[:, ti] - x0[:, 0]) > n_sigmas * s0)
        new_matrix[:, ti] = mask*x0[:, 0] + (1 - mask)*input_matrix[:, ti]

    return new_matrix


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('all_dir', help='All the files in the directory, default no', type=int, default=0)
    parser.add_argument('name', help='Name of experiment file')
    parser.add_argument('nss', help='Number of spatial streams', type=int)
    parser.add_argument('ncore', help='Number of cores', type=int)
    parser.add_argument('start_idx', help='Idx where start processing for each stream', type=int)
    args = parser.parse_args()

    exp_dir = args.dir
    names = []

    if args.all_dir:
        all_files = listdir(exp_dir)
        mat_files = []
        for i in range(len(all_files)):
            if all_files[i].endswith('.mat'):
                names.append(all_files[i][:-4])
    else:
        names.append(args.name)

    for name in names:
        name_file = './phase_processing/signal_' + name + '.txt'
        if path.exists(name_file):
            #print('Already processed')
            continue

        csi_buff_file = exp_dir + name + ".mat"
        csi_buff = sio.loadmat(csi_buff_file)
        csi_buff = (csi_buff['csi_buff'])
        csi_buff = np.fft.fftshift(csi_buff, axes=1)

        delete_idxs = np.argwhere(np.sum(csi_buff, axis=1) == 0)[:, 0]
        csi_buff = np.delete(csi_buff, delete_idxs, axis=0)

        delete_idxs = np.asarray([0, 1, 2, 3, 4, 5, 127, 128, 129, 251, 252, 253, 254, 255], dtype=int)

        n_ss = args.nss
        n_core = args.ncore
        n_tot = n_ss * n_core

        start = args.start_idx  # 1000
        end = int(np.floor(csi_buff.shape[0]/n_tot))
        signal_complete = np.zeros((csi_buff.shape[1] - delete_idxs.shape[0], end-start, n_tot), dtype=complex)

        for stream in range(0, n_tot):
            signal_stream = csi_buff[stream:end*n_tot + 1:n_tot, :][start:end, :]
            signal_stream[:, 64:] = - signal_stream[:, 64:]

            signal_stream = np.delete(signal_stream, delete_idxs, axis=1)
            mean_signal = np.mean(np.abs(signal_stream), axis=1, keepdims=True)
            H_m = signal_stream/mean_signal

            signal_complete[:, :, stream] = H_m.T

        name_file = './phase_processing/signal_' + name + '.txt'
        with open(name_file, "wb") as fp:  # Pickling
            pickle.dump(signal_complete, fp)
```

./CSI_phase_sanitization_signal_reconstruction.py:
```

"""
    Copyright (C) 2022 Francesca Meneghello
    contact: meneghello@dei.unipd.it
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import argparse
import numpy as np
import scipy.io as sio
from os import listdir, path
import pickle
import math as mt
import os

def is_valid_pickle(path):
    try:
        with open(path, 'rb') as f:
            pickle.load(f)
        return True
    except:
        return False

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('dir', help='Directory of data')
    parser.add_argument('dir_save', help='Directory to save processed data')
    parser.add_argument('nss', help='Number of spatial streams', type=int)
    parser.add_argument('ncore', help='Number of cores', type=int)
    parser.add_argument('start_idx', help='Start index', type=int)
    parser.add_argument('end_idx', help='End index from the end', type=int)
    args = parser.parse_args()

    exp_dir = args.dir
    save_dir = args.dir_save
    names = []

    all_files = listdir(exp_dir)
    for i in range(len(all_files)):
        if all_files[i].startswith('Tr') and all_files[i].endswith('.txt'):
            names.append(all_files[i][:-4])

    for name in names:
        name_f = name[10:] + '.mat'
        stop = False
        sub_dir_name = name_f.split('_stream')[0]
        subdir_path = save_dir + sub_dir_name

        complete_path = subdir_path + '/' + name_f
        #print(complete_path)
        if path.isfile(complete_path):
            stop = True

        if stop:
            #print('Already processed')
            continue

        if not os.path.exists(subdir_path):
            os.mkdir(subdir_path)

        name_file_save = subdir_path + '/' + name_f
        name_file = exp_dir + name + '.txt'
        if not is_valid_pickle(name_file):
            print(f"Corrupted pickle detected: {name_file}")
            os.remove(name_file)  # Remove corrupted file
            continue
        with open(name_file, "rb") as fp:  # Unpickling
            H_est = pickle.load(fp)

        end_H = H_est.shape[1]
        H_est = H_est[:, args.start_idx:end_H-args.end_idx]
        F_frequency = 256
        csi_matrix_processed = np.zeros((H_est.shape[1], F_frequency, 2))

        # AMPLITUDE
        csi_matrix_processed[:, 6:-5, 0] = np.abs(H_est[6:-5, :]).T

        # PHASE
        phase_before = np.unwrap(np.angle(H_est[6:-5, :]), axis=0)
        phase_err_tot = np.diff(phase_before, axis=1)
        ones_vector = np.ones((2, phase_before.shape[0]))
        ones_vector[1, :] = np.arange(0, phase_before.shape[0])
        for tidx in range(1, phase_before.shape[1]):
            stop = False
            idx_prec = -1
            while not stop:
                phase_err = phase_before[:, tidx] - phase_before[:, tidx - 1]
                diff_phase_err = np.diff(phase_err)
                idxs_invert_up = np.argwhere(diff_phase_err > 0.9 * mt.pi)[:, 0]
                idxs_invert_down = np.argwhere(diff_phase_err < -0.9 * mt.pi)[:, 0]
                if idxs_invert_up.shape[0] > 0:
                    idx_act = idxs_invert_up[0]
                    if idx_act == idx_prec:  # to avoid a continuous jump
                        stop = True
                    else:
                        phase_before[idx_act + 1:, tidx] = phase_before[idx_act + 1:, tidx] \
                                                           - 2 * mt.pi
                        idx_prec = idx_act
                elif idxs_invert_down.shape[0] > 0:
                    idx_act = idxs_invert_down[0]
                    if idx_act == idx_prec:
                        stop = True
                    else:
                        phase_before[idx_act + 1:, tidx] = phase_before[idx_act + 1:, tidx] \
                                                           + 2 * mt.pi
                        idx_prec = idx_act
                else:
                    stop = True
        for tidx in range(1, H_est.shape[1] - 1):
            val_prec = phase_before[:, tidx - 1:tidx]
            val_act = phase_before[:, tidx:tidx + 1]
            error = val_act - val_prec
            temp2 = np.linalg.lstsq(ones_vector.T, error)[0]
            phase_before[:, tidx] = phase_before[:, tidx] - (np.dot(ones_vector.T, temp2)).T

        csi_matrix_processed[:, 6:-5, 1] = phase_before.T

        mdic = {"csi_matrix_processed": csi_matrix_processed[:, 6:-5, :]}
        sio.savemat(name_file_save, mdic)
```

./Makefile:
```
# Define variables for directories and parameters
INPUT_DIR = ../input_files/
OUTPUT_DIR = ./processed_phase/
DOPPLER_DIR = ./doppler_traces/
NSS = 1  # Number of spatial streams
NCORE = 4  # Number of cores
START_IDX = 0  # Index to start processing
SAMPLE_LENGTH = 31  # Number of packets in a sample
SLIDING = 1  # Number of packets for sliding operations
NOISE_LEVEL = -1.2  # Noise level for thresholding
WINDOWS_LENGTH = 340  # Number of samples per window
STRIDE_LENGTHS = 30  # Number of samples for window sliding
FEATURE_LENGTH = 100  # Length along the feature dimension (height)
BATCH_SIZE = 16  # Number of samples in a batch
NAME_BASE = single_ant.keras   # Name prefix for the files
ACTIVITIES = C,C1,C2,E,E1,E2,H,H1,H2,J,J1,J2,J3,L,L1,L2,L3,R,R1,R2,S,W,W1,W2  # Activities to be considered
NUM_TOT = 4
# Undersampling parameters
ENABLE_UNDERSAMPLE = --undersample  # Set to empty string to disable
UNDERSAMPLE_RATIO = 1.0  # 1.0 = fully balanced, 0.0 = no balancing

# Define the commands for each step
PREPROCESS_CMD = for subdir in $(INPUT_DIR)AR-*; do \
    dir_name=$$(basename "$$subdir"); \
    python CSI_phase_sanitization_signal_preprocessing.py "$$subdir/" 1 - $(NSS) $(NCORE) $(START_IDX); \
done
H_ESTIMATION_CMD = python parallel_h_estimation.py
RECONSTRUCTION_CMD = for subdir in $(INPUT_DIR)AR-*; do \
    dir_name=$$(basename "$$subdir"); \
    python CSI_phase_sanitization_signal_reconstruction.py ./phase_processing/ $(OUTPUT_DIR) $(NSS) $(NCORE) $(START_IDX) -1; \
done
SUBDIRS_DOPPLER = AR7a_J3,AR8a_J2,AR9a_J2,AR3a_R,AR6a_W2,AR8b_L1,AR1a_S,AR8b_J2,AR1d_S,AR1a_E,AR2a_J1,AR9b_W2,AR5a_R,AR2a_J2,AR5a_J2,AR9c_J2,AR1c_S,AR1c_H,AR6a_R2,AR1e_J1,AR5a_L,AR1e_L,AR3a_S,AR9c_L1,AR6a_J1,AR8a_R2,AR4a_C1,AR9b_J1,AR8a_R1,AR7a_W,AR9a_J1,AR1c_W,AR5b_W,AR2a_H,AR9c_R2,AR4a_R,AR1b_E,AR1c_C,AR1b_J1,AR2a_L,AR9b_L3,AR8b_J1,AR2a_W,AR6a_L2,AR6a_W1,AR8b_W1,AR8b_W2,AR9a_L2,AR3b_R,AR3a_H1,AR9c_W1,AR7a_R,AR5b_J1,AR4a_J2,AR8b_E1,AR6a_E,AR1e_W,AR7a_L2,AR1d_C1,AR7a_S,AR8a_L1,AR3a_C2,AR1e_J2,AR1d_C2,AR4a_C2,AR8a_L3,AR7a_C,AR3b_E,AR3b_W,AR1d_E,AR9c_L3,AR3a_J1,AR1a_J1,AR3a_W,AR1d_J,AR1a_J2,AR4a_E,AR1a_C,AR1d_W,AR5a_H,AR6a_J2,AR9b_W1,AR3a_J2,AR7a_J2,AR3b_J1,AR1b_W,AR5a_C,AR8a_E1,AR2a_S,AR4a_W,AR5a_S,AR9c_J3,AR3b_L,AR8a_J1,AR5b_J2,AR1a_H,AR8a_W1,AR1c_E,AR1d_L,AR9a_L1,AR1a_W,AR9a_E,AR3b_J2,AR8a_W2,AR3a_L,AR1a_L,AR4a_J1,AR7a_H,AR4a_S,AR7a_J1,AR9a_W2,AR1c_J1,AR4a_H1,AR3a_C1,AR9b_L2,AR9b_J2,AR1b_S,AR5b_R,AR7a_L1,AR1c_L,AR9b_E,AR3a_H2,AR3a_E,AR8b_E2,AR8b_L2,AR6a_J3,AR9b_L1,AR5a_E,AR1b_L,AR2a_C,AR1a_R,AR9c_W2,AR8a_E2,AR9c_J1,AR6a_R1,AR9a_R1,AR9c_L2,AR9a_R2,AR7a_E,AR8a_L2,AR1d_H,AR2a_E,AR6a_L1,AR9a_W1,AR5b_L,AR1b_R,AR9b_R1,AR9c_E,AR1d_R,AR9c_R1,AR2a_R,AR1b_H,AR1c_R,AR1e_R,AR4a_L,AR5a_W,AR5a_J1,AR4a_H2,AR1c_J2,AR1e_E,AR1b_C,AR5b_E,AR9b_R2,AR1b_J2
DOPPLER_CMD = python CSI_doppler_computation.py $(OUTPUT_DIR) "$(SUBDIRS_DOPPLER)" $(DOPPLER_DIR) 800 800 $(SAMPLE_LENGTH) $(SLIDING) $(NOISE_LEVEL)
CREATE_DATASET_TRAIN_CMD = python CSI_doppler_create_dataset_train.py $(DOPPLER_DIR) $(SUBDIRS_TRAIN) $(SAMPLE_LENGTH) $(SLIDING) $(WINDOWS_LENGTH) $(STRIDE_LENGTHS) $(ACTIVITIES) 4
CREATE_DATASET_TEST_CMD = python CSI_doppler_create_dataset_test.py $(DOPPLER_DIR) $(SUBDIRS_TEST) $(SAMPLE_LENGTH) $(SLIDING) $(WINDOWS_LENGTH) $(STRIDE_LENGTHS) $(ACTIVITIES) 4
TRAIN_CMD = python CSI_network.py $(DOPPLER_DIR) $(SUBDIRS_TRAIN) $(FEATURE_LENGTH) $(WINDOWS_LENGTH) 1 $(BATCH_SIZE) $(NUM_TOT) $(NAME_BASE) $(ACTIVITIES) $(ENABLE_UNDERSAMPLE) --undersample_ratio=$(UNDERSAMPLE_RATIO)
TEST_CMD = python CSI_network_test.py $(DOPPLER_DIR) $(SUBDIRS_TEST) $(FEATURE_LENGTH) $(WINDOWS_LENGTH) 1 $(BATCH_SIZE) $(NUM_TOT) $(NAME_BASE) $(ACTIVITIES)

AR1d=AR1d_C1,AR1d_C2,AR1d_E,AR1d_H,AR1d_J,AR1d_L,AR1d_R,AR1d_S,AR1d_W

AR7a=AR7a_C,AR7a_E,AR7a_H,AR7a_J1,AR7a_J2,AR7a_J3,AR7a_L1,AR7a_L2,AR7a_R,AR7a_S,AR7a_W

AR3a=AR3a_C1,AR3a_C2,AR3a_E,AR3a_H1,AR3a_H2,AR3a_J1,AR3a_J2,AR3a_L,AR3a_R,AR3a_S,AR3a_W
AR4a=AR4a_C1,AR4a_C2,AR4a_E,AR4a_H1,AR4a_H2,AR4a_J1,AR4a_J2,AR4a_L,AR4a_R,AR4a_S,AR4a_W

AR9b=AR9b_E,AR9b_J1,AR9b_J2,AR9b_L1,AR9b_L2,AR9b_L3,AR9b_R1,AR9b_R2,AR9b_W1,AR9b_W2
AR6a=AR6a_E,AR6a_J1,AR6a_J2,AR6a_J3,AR6a_L1,AR6a_L2,AR6a_R1,AR6a_R2,AR6a_W1,AR6a_W2

AR8a=AR8a_E1,AR8a_E2,AR8a_J1,AR8a_J2,AR8a_L1,AR8a_L2,AR8a_L3,AR8a_R1,AR8a_R2,AR8a_W1,AR8a_W2
AR9a=AR9a_E,AR9a_J1,AR9a_J2,AR9a_L1,AR9a_L2,AR9a_R1,AR9a_R2,AR9a_W1,AR9a_W2

AR8b=AR8b_E1,AR8b_E2,AR8b_J1,AR8b_J2,AR8b_L1,AR8b_L2,AR8b_W1,AR8b_W2

AR1a=AR1a_C,AR1a_E,AR1a_H,AR1a_J1,AR1a_J2,AR1a_L,AR1a_R,AR1a_S,AR1a_W

AR2a=AR2a_C,AR2a_E,AR2a_H,AR2a_J1,AR2a_J2,AR2a_L,AR2a_R,AR2a_S,AR2a_W
AR5a=AR5a_C,AR5a_E,AR5a_H,AR5a_J1,AR5a_J2,AR5a_L,AR5a_R,AR5a_S,AR5a_W
AR1c=AR1c_C,AR1c_E,AR1c_H,AR1c_J1,AR1c_J2,AR1c_L,AR1c_R,AR1c_S,AR1c_W
AR1b=AR1b_C,AR1b_E,AR1b_H,AR1b_J1,AR1b_J2,AR1b_L,AR1b_R,AR1b_S,AR1b_W

AR9c=AR9c_E,AR9c_J1,AR9c_J2,AR9c_J3,AR9c_L1,AR9c_L2,AR9c_L3,AR9c_R1,AR9c_R2,AR9c_W1,AR9c_W2

AR1e=AR1e_E,AR1e_J1,AR1e_J2,AR1e_L,AR1e_R,AR1e_W
AR5b=AR5b_E,AR5b_J1,AR5b_J2,AR5b_L,AR5b_R,AR5b_W
AR3b=AR3b_E,AR3b_J1,AR3b_J2,AR3b_L,AR3b_R,AR3b_W


bedroom = $(AR1a),$(AR1b),$(AR1c),$(AR1d),$(AR1e),$(AR2a),$(AR3a),$(AR3b),$(AR4a)
living_room = $(AR5a),$(AR5b)
kitchen = $(AR6a)
laboratory = $(AR7a)
office = $(AR8a),$(AR8b)
semi_anechoic_chamber = $(AR9a),$(AR9b),$(AR9c)
clean:
	python clean_dataset_train.py
data_processing:
	$(PREPROCESS_CMD)
	$(H_ESTIMATION_CMD)
	$(RECONSTRUCTION_CMD)
	$(DOPPLER_CMD)
run_nn:
	$(TRAIN_CMD)
	$(TEST_CMD)

# Toggle undersampling on/off
enable_undersampling:
	$(eval ENABLE_UNDERSAMPLE = --undersample)
	@echo "Undersampling enabled with ratio $(UNDERSAMPLE_RATIO)"

disable_undersampling:
	$(eval ENABLE_UNDERSAMPLE = )
	@echo "Undersampling disabled"

# Set undersampling ratio
set_undersample_ratio_full:
	$(eval UNDERSAMPLE_RATIO = 1.0)
	@echo "Undersampling ratio set to full balance (1.0)"

set_undersample_ratio_partial:
	$(eval UNDERSAMPLE_RATIO = 0.7)
	@echo "Undersampling ratio set to partial balance (0.7)"

set_undersample_ratio_minimal:
	$(eval UNDERSAMPLE_RATIO = 0.3)
	@echo "Undersampling ratio set to minimal balance (0.3)"

# Run with undersampling
run_nn_with_undersampling: enable_undersampling
	$(TRAIN_CMD)
	$(TEST_CMD)

run_nn_with_partial_undersampling: enable_undersampling set_undersample_ratio_partial
	$(TRAIN_CMD)
	$(TEST_CMD)

# Define the targets for each variation
no_unseen_domains:
	$(eval SUBDIRS_TRAIN = AR1a_S,AR1b_E,AR1c_C,AR3a_R,AR4a_R,AR5a_C,AR6a_E,AR7a_J1,AR8a_E1,AR9a_J1,AR9b_J1)
	$(eval SUBDIRS_TEST = AR1d_S,AR1e_E,AR2a_J1,AR5b_R,AR8b_E1,AR9c_J1)
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
no_unseen_domains_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = AR1a_S,AR1b_E,AR1c_C,AR3a_R,AR4a_R,AR5a_C,AR6a_E,AR7a_J1,AR8a_E1,AR9a_J1,AR9b_J1)
	$(eval SUBDIRS_TEST = AR1d_S,AR1e_E,AR2a_J1,AR5b_R,AR8b_E1,AR9c_J1)
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_bedroom_hidden:
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(bedroom))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_bedroom_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(bedroom))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_living_room_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_living_room_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_kitchen_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(living_room),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(kitchen))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_kitchen_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(living_room),$(laboratory),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(kitchen))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_laboratory_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(living_room),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_laboratory_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(living_room),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_office_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(living_room),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(office))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_office_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(living_room),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(office))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

one_unseen_domain_semi_anechoic_chamber_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(office),$(living_room))
	$(eval SUBDIRS_TEST = $(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
one_unseen_domain_semi_anechoic_chamber_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(office),$(living_room))
	$(eval SUBDIRS_TEST = $(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

multiple_unseen_domains_bedroom_and_semi_anechoic_chamber_hidden:
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory),$(office),)
	$(eval SUBDIRS_TEST = $(bedroom),$(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
multiple_unseen_domains_bedroom_and_semi_anechoic_chamber_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory),$(office),)
	$(eval SUBDIRS_TEST = $(bedroom),$(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

multiple_unseen_domains_kitchen_and_laboratory_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(living_room),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(kitchen),$(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
multiple_unseen_domains_kitchen_and_laboratory_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(living_room),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(kitchen),$(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

multiple_unseen_domains_living_room_and_office_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room),$(office))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
multiple_unseen_domains_living_room_and_office_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(kitchen),$(laboratory),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room),$(office))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

multiple_unseen_domains_office_bedroom_and_semi_anechoic_chamber_hidden:
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory))
	$(eval SUBDIRS_TEST = $(office),$(bedroom),$(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
multiple_unseen_domains_office_bedroom_and_semi_anechoic_chamber_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(living_room),$(kitchen),$(laboratory))
	$(eval SUBDIRS_TEST = $(office),$(bedroom),$(semi_anechoic_chamber))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

multiple_unseen_domains_living_room_kitchen_and_laboratory_hidden:
	$(eval SUBDIRS_TRAIN = $(bedroom),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room),$(kitchen),$(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

# Version with undersampling
multiple_unseen_domains_living_room_kitchen_and_laboratory_hidden_undersampled: enable_undersampling
	$(eval SUBDIRS_TRAIN = $(bedroom),$(office),$(semi_anechoic_chamber))
	$(eval SUBDIRS_TEST = $(living_room),$(kitchen),$(laboratory))
	$(CREATE_DATASET_TRAIN_CMD)
	$(CREATE_DATASET_TEST_CMD)
	$(TRAIN_CMD)
	$(TEST_CMD)

all: data_processing all_tests
all_tests: 
	 no_unseen_domains one_unseen_domain_bedroom_hidden one_unseen_domain_living_room_hidden \
	 one_unseen_domain_kitchen_hidden one_unseen_domain_laboratory_hidden one_unseen_domain_office_hidden \
	 one_unseen_domain_semi_anechoic_chamber_hidden multiple_unseen_domains_bedroom_and_semi_anechoic_chamber_hidden \
	 multiple_unseen_domains_equivalent_data_kitchen_laboratory_hidden \
	 multiple_unseen_domains_equivalent_data_kitchen_laboratory_hidden_test2

# Run all tests with undersampling enabled
all_tests_undersampled: enable_undersampling
	 no_unseen_domains one_unseen_domain_bedroom_hidden one_unseen_domain_living_room_hidden \
	 one_unseen_domain_kitchen_hidden one_unseen_domain_laboratory_hidden one_unseen_domain_office_hidden \
	 one_unseen_domain_semi_anechoic_chamber_hidden multiple_unseen_domains_bedroom_and_semi_anechoic_chamber_hidden \
	 multiple_unseen_domains_equivalent_data_kitchen_laboratory_hidden \
	 multiple_unseen_domains_equivalent_data_kitchen_laboratory_hidden_test2

# Run individual test scenarios with different undersampling ratios
run_with_full_undersampling: set_undersample_ratio_full enable_undersampling
	$(TRAIN_CMD)
	$(TEST_CMD)

run_with_partial_undersampling: set_undersample_ratio_partial enable_undersampling
	$(TRAIN_CMD)
	$(TEST_CMD)

run_with_minimal_undersampling: set_undersample_ratio_minimal enable_undersampling
	$(TRAIN_CMD)
	$(TEST_CMD)```

